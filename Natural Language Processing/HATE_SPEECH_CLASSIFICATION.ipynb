{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LEO PAGGEN i6236337**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bxz9jMN4JqQi",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# NLP 2024\n",
        "# Lab 1: Classification models: Detecting hate speech\n",
        "\n",
        "The rise of social media has allowed the expression of opinions\n",
        "about debatable topics. X (ex Twitter) has emerged as a strong channel of communication to gather and disseminate news, to forecast outcome of elections and to exchange political events and discussions. It has also become an  important analytical tool for crime forecasting, tracking terrorist activities and detecting hate speech.\n",
        "\n",
        "Hate speech is commonly defined as \"any message that mocks\n",
        "or discriminates against a person or group based on specific\n",
        "characteristics such as color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics\". The\n",
        "amount of hate speech is steadily increasing due to X's\n",
        "popularity and the resulting big data from user-generated content.\n",
        "\n",
        "While NLP presents a promising approach to measure, detect and fight hate speech, that sounds more easy than it is. First of all, think about all the challenges that come with language evolution and language use in practice (e.g. sarcasm, slang, cultural variations) in understanding the true intent behind text. Moreover, there is an active discussion on what exactly consitutes hate speech from a legal perspective. Read more on this [here](https://aclanthology.org/2022.nllp-1.5.pdf).\n",
        "\n",
        "In this lab, we will explore the linguistic, technical and ethical spects of treating hate speech identification as a NLP classification task.\n",
        "\n",
        "By the end of this lab you should be able to:\n",
        "- Implement and/or use built-in functions to preprocess your data\n",
        "- Implement simple classification pipelines\n",
        "- Import and use `huggingface datasets` library\n",
        "- Import and use functions from `sklearn`\n",
        "- Evaluate classification results\n",
        "- Assess the difficulty of specific NLP tasks and propose solutions\n",
        "- Reflect on the ethical dimensions that a NLP model can have\n",
        "\n",
        "### Score breakdown\n",
        "\n",
        "Exercise | Points\n",
        "--- | ---\n",
        "[Exercise 1](#e1) | 7\n",
        "[Exercise 2](#e2) | 2\n",
        "[Exercise 3](#e3) | 2\n",
        "[Exercise 4](#e4) | 2\n",
        "[Exercise 5](#e5) | 5\n",
        "[Exercise 6](#e6) | 4\n",
        "[Exercise 7](#e7) | 5\n",
        "[Exercise 8](#e8) | 5\n",
        "[Exercise 9](#e9) | 5\n",
        "[Exercise 10](#e10) | 10\n",
        "[Exercise 11](#e11) | 3\n",
        "[Exercise 12](#e12) | 10\n",
        "[Exercise 13](#e13) | 5\n",
        "[Exercise 14](#e14) | 10\n",
        "[Exercise 15](#e15) | 10\n",
        "[Exercise 16](#e16) | 10\n",
        "[Exercise 17](#e17) | 5\n",
        "Total | 100\n",
        "\n",
        "This score will be scaled down to 1 and that will be your final lab score.\n",
        "\n",
        "### Instructions for delivery (Deadline: 6/May late night, wildcards possible)\n",
        "\n",
        "+ Make sure that you include a proper amount/mix of comments, results and code.\n",
        "+ In the end, make sure that all cells are executed properly and everything you need to show is in your (execucted) notebook.\n",
        "+ You are asked to deliver only your executed notebook file, .ipnyb and nothing else. Enjoy!\n",
        "+ Honor code applies to these tasks. Only individual work should be submitted.\n",
        "+ While you may talk with others about this lab, we ask that you write your solutions individually. If you do discuss specific tasks with others please include their names below.\n",
        "+ It is mandatory to list and disclose any website (or other resource) you used (e.g. stackoverflow) as well as any genAI tools (e.g. chatGPT) used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USvThR914dyx"
      },
      "source": [
        "Collaborators: list collaborators here\n",
        "\n",
        "**I talked with Jerry about...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EuzalFF4dgg"
      },
      "source": [
        "Use of genAI tools (e.g. chatGPT), websites (e.g. stackoverflow): list websites where you found code (or other info) as well as include information on how you used genAI tools (e.g. prompts):\n",
        "\n",
        "I asked chatGPT about..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0cN4xHB1JqQj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Outline\n",
        "- [0 Setup](#0)\n",
        "- [1: Load Dataset](#1)\n",
        "- [2: Preprocess Dataset](#2)\n",
        "  - [Exercise 1](#e1)\n",
        "- [3: Build Vocabulary](#3)\n",
        "  - [Exercise 2](#e2)\n",
        "  - [Exercise 3](#e3)\n",
        "  - [Exercise 4](#e4)\n",
        "  - [Exercise 5](#e5)\n",
        "  - [Exercise 6](#e6)\n",
        "- [4: Build Handcrafted Classifier](43)\n",
        "  - [Exercise 7](#e7)\n",
        "  - [Exercise 8](#e8)\n",
        "- [5: Build Bag-of-Words](#5)\n",
        "  - [Exercise 9](#e9)\n",
        "- [6: Bag-of-Words with Naive Bayes](#6)\n",
        "  - [Exercise 10](#e10)\n",
        "- [7: Bag-of-Words with Logistic Regression](#7)\n",
        "  - [Exercise 11](#e11)\n",
        "  - [Exercise 12](#e12)\n",
        "- [8: TF-IDF](#8)\n",
        "  - [Exercise 13](#e13)\n",
        "  - [Exercise 14](#e14)\n",
        "- [9: Adding Handcrafted Features](#9)\n",
        "  - [Exercise 15](#e15)\n",
        "  - [Exercise 16](#e16)\n",
        "- [10: Reflection, Bias, Fairness, Ethics](#10)\n",
        "  - [Exercise 17](#e17)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GUl1lKN2JqQj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='0'></a>\n",
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lbkqBA_9JqQj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We will be using huggingface datasets library ([https://huggingface.co/datasets](https://huggingface.co/datasets)). You can find the detailed documentation and tutorials here: [https://huggingface.co/docs/datasets/en/index](https://huggingface.co/docs/datasets/en/index)\n",
        "\n",
        "If you don't have it installed you can run the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-LrHJyhBJqQj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets~=2.18.0 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (1.24.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (16.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets~=2.18.0) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (0.22.2)\n",
            "Requirement already satisfied: packaging in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from datasets~=2.18.0) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from aiohttp->datasets~=2.18.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from aiohttp->datasets~=2.18.0) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from aiohttp->datasets~=2.18.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from aiohttp->datasets~=2.18.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from aiohttp->datasets~=2.18.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from aiohttp->datasets~=2.18.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from huggingface-hub>=0.19.4->datasets~=2.18.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from requests>=2.19.0->datasets~=2.18.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from requests>=2.19.0->datasets~=2.18.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from requests>=2.19.0->datasets~=2.18.0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from requests>=2.19.0->datasets~=2.18.0) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from pandas->datasets~=2.18.0) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from pandas->datasets~=2.18.0) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from pandas->datasets~=2.18.0) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets~=2.18.0) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U datasets~=2.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "n-J0bd6hJqQj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We will also need those packages, so let's import them now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9E-maoXPJqQk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/lpaggen/miniforge3/envs/tensorflow/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import re    # for regular expressions\n",
        "from string import punctuation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dNyZiYIEJqQk",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1. Load Dataset\n",
        "\n",
        "We will be using \"tweet_eval\" dataset and specifically the hate speech subset. You can find more info on how this dataset has been created in the following papers [[1](https://aclanthology.org/S19-2007.pdf)],[[2](https://aclanthology.org/2020.findings-emnlp.148.pdf)]. The dataset contains tweets from 2018 and the goal is to identify whether a tweet is hateful or not against any of the two target communities: immigrants and women. Make sure to read and reflect on how annotation was performed, since this can give you some insights on the modeling.\n",
        "\n",
        "In order to load a dataset simply call ```load_dataset``` function specifying the dataset name. You can find many more datasets at the huggingface website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ljTlpzURJqQk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 9000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = datasets.load_dataset('tweet_eval', 'hate')\n",
        "print(tweet_ds)\n",
        "\n",
        "# this will make the dataset return the values as numpy arrays\n",
        "tweet_ds = tweet_ds.with_format(\"np\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-M-GMrZqJqQm",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The loaded dataset contains three subsets (\"train\", \"validation\", and \"test\"). Each consists of two columns: \"text\" and \"label\". Label of 0 means \"non-hate\" and label of 1 means \"hate\" We can access them like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NbCnVRuNJqQm",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on you…', 'label': 0}\n",
            "{'text': 'A woman who you fucked multiple times saying yo dick small is a compliment you know u hit that spot 😎', 'label': 1}\n",
            "{'text': '@user @user real talk do you have eyes or were they gouged out by a rapefugee?', 'label': 1}\n",
            "{'text': 'your girlfriend lookin at me like a groupie in this bitch!', 'label': 1}\n",
            "{'text': 'Hysterical woman like @user', 'label': 0}\n",
            "{'text': 'Me flirting- So tell me about your father...', 'label': 0}\n",
            "{'text': 'The Philippine Catholic bishops\\' work for migrant workers should focus on families who are \"paying the great...', 'label': 0}\n",
            "{'text': \"I AM NOT GOING AFTER YOUR EX BF YOU LIEING SACK OF SHIT ! I'm done with you dude that's why I dumped your ass cause your a lieing 😂😡 bitch\", 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "for i in range(8):\n",
        "    print(tweet_ds['train'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2BH8vvzqJqQm",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "as part of the dataset inspection, it might be helpful to help them extract it and actually see all data points\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "W9ijz8qNJqQm",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "You can easily cast the dataset to the pandas DataFrame. We will do that below to plot the balance of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EwcVYkb7JqQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='label'>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pd.DataFrame(tweet_ds['train']).groupby('label').count().plot.bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "h8NhtvljJqQn",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can filter the examples based on the label. We will use ```filter()``` method. See this link for more details [https://huggingface.co/docs/datasets/en/use_dataset](https://huggingface.co/docs/datasets/en/use_dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RqX3YWMEJqQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on you…', 'label': 0}\n",
            "{'text': 'Hysterical woman like @user', 'label': 0}\n",
            "{'text': 'Me flirting- So tell me about your father...', 'label': 0}\n",
            "{'text': 'The Philippine Catholic bishops\\' work for migrant workers should focus on families who are \"paying the great...', 'label': 0}\n",
            "{'text': 'When cuffin season is finally over', 'label': 0}\n",
            "{'text': \"Only that in you which is me can hear what I'm saying. ~Baba Ram Dass\", 'label': 0}\n",
            "{'text': 'More migrants take sea route to #Spain than Italy this year: UN', 'label': 0}\n",
            "{'text': 'Essential reading for those in Scribner, Nebraska who are considering an anti-immigrant ballot measure.', 'label': 0}\n",
            "{'text': 'RT @user @user I am flattered tbh. I got an orgasm just by thinking of it you really are good at this.', 'label': 0}\n",
            "{'text': 'Making them look ~anatomically correct~ just makes them... Bland. Not all women are tiny and fit, not all men are bulky.', 'label': 0}\n",
            "{'text': 'An Honest Article About Illegal Immigration By a Lefty?? Are Pigs Flying? -     A miracle...', 'label': 0}\n",
            "{'text': '@user @user @user @user please spread the word  -&gt;&gt; using frequent flyer miles to reunite immigrant families!', 'label': 0}\n",
            "{'text': \"Yes, women who protest are 'irrational' 'hysterical' 'bitters spinsters' - long history of using that shit to dismi…\", 'label': 0}\n",
            "{'text': '@user Not all men are animals. You Libs want muslims to come over still? Better pick a side.', 'label': 0}\n",
            "{'text': 'Federal judge to #Trump: \"imminent danger\" of \"irreparable injury to #refugees, visa-holders\", etc. @user', 'label': 0}\n",
            "{'text': 'Why we need to protect refugees from the ‘˜big ideas’ designed to save them @user @user @user @user @user @user @user', 'label': 0}\n",
            "{'text': 'Kevin Rosero of the Latino Immigrant Experience Oral History Project #NJHRIC #NewarkPublicLibrary needs help identifying Newark Latino immigrant residents to interview and capture their stories and experiences. MORE INFO:', 'label': 0}\n",
            "{'text': '@user Are you pussy grabbing tonight??', 'label': 0}\n",
            "{'text': \"Some people really dont understand why I get so upset at injustices happening in this country. Trust me, it's not that I like getting my panties in a wad over dumb things. It's the fact that people like me (minority, brown, immigrant, etc) r suffering and it affects me directly\", 'label': 0}\n",
            "{'text': \"I've been called a son of a whore and a cunt today already. It's not even 1 yet\", 'label': 0}\n",
            "{'text': 'Pope Francis: Immigrants Provide ‘˜Enrichment’ to Society, Not ‘˜Threats’', 'label': 0}\n",
            "{'text': 'What were they planning to do with the immigrant children they kidnapped? Slavery, sex traffic, organ harvesting?', 'label': 0}\n",
            "{'text': 'masturbate intensive for that whore slut', 'label': 0}\n",
            "{'text': 'The Truth about #Immigration', 'label': 0}\n",
            "{'text': 'Lebanese Christian victim of a quarrel between Syrian refugees carrying knives &amp; cleavers in Lebanon', 'label': 0}\n",
            "{'text': 'CoreCivic found its best interest is to grow US detention capacity, for women and children, by over 450%. Expanding the number of mother-child beds from 3,500 up to 20,000. This signals the largest increase in immigrant detention since WW2 #ResistICENashville photo: Julie Henry', 'label': 0}\n",
            "{'text': 'I wonder is this is where Caitlyn gets his car services 🌚', 'label': 0}\n",
            "{'text': 'Bishop appeals to church groups to take action as migrants now account for 11 percent of reported cases', 'label': 0}\n",
            "{'text': '@user I am not forcing anyone. I am just putting out my view. People believe it or not is their prerogative.Even if a single person agree with me, it is ok with me. Thanks,', 'label': 0}\n",
            "{'text': '#FactCheck 👍 #BoycottUnitedArlines #AidingAndAbetting #Felons #WeThePeople are FED UP #EnoughIsEnough #ICE #DHS #DOJ #WakeUpAmerica #StopTheInvasion', 'label': 0}\n",
            "{'text': '#NRC Assam Govt must initiate immediate action 2 cancel Voting id cards &amp; Aadhaar cards of the identified illegal migrants Let \"commited\" political dogs bark any length &amp; as loud as they can Nation must remain firm on its security concerns', 'label': 0}\n",
            "{'text': 'Russia, Turkey, Jordan are in talks on return of Syrian refugees ‘“ Lavrov', 'label': 0}\n",
            "{'text': 'NEVERRRRRRRRRRR', 'label': 0}\n",
            "{'text': \"Thank you, @user for taking a stand. It's up to all of us to fight misinformation and anti-immigrant hate. #SourcesMatter\", 'label': 0}\n",
            "{'text': 'Realizing the rights of Eritrean refugees in Israel. A new article:https://t.co/0SsTUvgQ6z', 'label': 0}\n",
            "{'text': 'Judge orders government to release immigrant kids from Texas shelter | The Texas Tribune', 'label': 0}\n",
            "{'text': \"@user @user @user Don't dare to tell us about Syrian refugees.\", 'label': 0}\n",
            "{'text': 'This is what Trump supporters call humane smh', 'label': 0}\n",
            "{'text': \"#OccupyICEPHL was delighted to learn that Charlie Kirk, the anti-immigrant, anti-Muslim, conspiracy theory-touting, founder &amp; executive director of the white nationalist-allied Turning Point USA (TPUSA) didn't have a very good morning either. #NationalistsNotWelcome\", 'label': 0}\n",
            "{'text': 'Med not a house/EU not adult day care camp for migrants seeking handouts or to profit from crime.Tunisia abusing #Italy EU, which has leverage bt lacks courage to use it. #Salvini must send messageðŸ‘‰either stop boats &amp; take back citizens or we bury you economically, esp. leaders', 'label': 0}\n",
            "{'text': '@user good girl', 'label': 0}\n",
            "{'text': '@user Greedy, you have to talk about this tomorrow including the hysterical responses.', 'label': 0}\n",
            "{'text': 'World Relief is continuing to provide legal aid and advocating for immigrant families who have been separated at the U.S.-Mexico border. You can make a difference and help reunite families by making a donation today. âž\\x9d', 'label': 0}\n",
            "{'text': 'From refugee to sole survivor - How one Syrian has become shoemaker to the famous', 'label': 0}\n",
            "{'text': \"When you're sneaking in after a long, hard night of cheating and don't wanna wake your girl cause you're a good guy\", 'label': 0}\n",
            "{'text': '@user sweetie, how the hell can you take “oh my god that’s a DICK?!?!?!?!” Style monologue', 'label': 0}\n",
            "{'text': 'Hundreds of Syrian refugees return home from Lebanon - ABC News', 'label': 0}\n",
            "{'text': 'VIDEO: Immigrant activist who climbed Statue of Liberty has a new song: \"America, you motherfuckers,...', 'label': 0}\n",
            "{'text': \"RT @user I bet a lot of people's therapists are dying to meet me just to see if the stories are true.\", 'label': 0}\n",
            "{'text': \"RT @user 'You miss 100% of the shots you don't take.' -- Wayne Gretzky\", 'label': 0}\n"
          ]
        }
      ],
      "source": [
        "no_hate = tweet_ds['train'].filter(lambda e: e['label'] == 0)\n",
        "for i in range(50):\n",
        "    print(no_hate[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "piHOC8PXJqQn",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's use the ```filter()``` method to remove empty entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xLcib2x5JqQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.filter(lambda e: e['text'] != '' and e['text'] is not None)\n",
        "print(tweet_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uE-RHPN2JqQn",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2. Preprocess Dataset\n",
        "\n",
        "In this section we will preprocess the dataset by cleaning and tokenizing the entries. You will use the knowledge and skills from the previous lab.\n",
        "\n",
        "Datasets library contains a very useful method ```map```. It expects a function that will receive an example from the dataset. This function will be applied to all entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "WTuR0bMNJqQo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e1'></a>\n",
        "### Exercise 1 (points 7)\n",
        "\n",
        "Fill in the function in order to clean the examples. We already provide some pre-processing (e.g. turn text to a lower case) and some regular expressions to handle some punctuation special cases, however the goal is for you to implement your own operations. You might want to inspect the dataset (e.g. check several data points) before you implement this method and also you might need to revisit it after you have seen the first classification results. For every operation, try to also include the intuition behind it (unless it's obvious).\n",
        "\n",
        "There is no exclusive list of pre-processing you need to have (since many decisions will depend on your observations). Try to include different examples of handling different language issues with specific regular expressions or other pre-processing decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T2EmO2qmJqQo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def clean(example):\n",
        "    \"\"\"\n",
        "    Cleans the example from the Dataset\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: update example containing 'clean' column\n",
        "\n",
        "    \"\"\"\n",
        "    text = example['text']\n",
        "\n",
        "    # Empty text\n",
        "    if type(text) not in (str, np.str_) or text=='':\n",
        "        example['clean'] = ''\n",
        "        return example\n",
        "\n",
        "    # 'text' from the example can be of type numpy.str_, let's convert it to a python str\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
        "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ve\", \" have \", text)\n",
        "\n",
        "    #you might need more\n",
        "    #add them here\n",
        "    text = re.sub(\"…\", \" . \", text)\n",
        "    text = re.sub(r'\\.\\.\\.(?=\\s|$)', ' .', text)\n",
        "    text = re.sub(\" \\\" \", \" \", text)\n",
        "    text = re.sub(\" cuffin \", \" cuffing \", text)\n",
        "    text = re.sub(r'(?<=\\w)-|-(?=\\w)', '  ', text)\n",
        "    text = re.sub(r'#\\w+', 'HASHTAG', text)\n",
        "    text = re.sub(r'@\\w+', 'PERSON', text)\n",
        "    text = re.sub(r'(?<=\\w)\\\\', '', text)\n",
        "    text = re.sub(r'(?=\\w)\\\"|\\\"(?=\\w)', '', text)\n",
        "    text = re.sub(r'(?=\\w)\\'', '', text)\n",
        "    text = re.sub(r'(?<=\\w)-(?=\\w)', '  ', text)\n",
        "    text = re.sub(r'\\~(?=\\w)', '', text)\n",
        "    text = re.sub(r'\\.', ' .', text)\n",
        "    text = re.sub(r',', ' , ', text)\n",
        "    text = re.sub(\" don't \", \" do not \", text)\n",
        "    text = re.sub(\" dont \", \" do not \", text)\n",
        "    text = re.sub(r'\\?+', ' ?', text)\n",
        "    text = re.sub(r'!+', '!', text)\n",
        "    text = re.sub(\"i'll\", \" i will \", text)\n",
        "    text = re.sub(\" r \", \" are \", text)\n",
        "    text = re.sub(\" you're \", \" you are \", text)\n",
        "    text = re.sub(r'(?=\\w)\\:', ' : ', text)\n",
        "    text = re.sub(r'\\~\\w+', '', text)\n",
        "    text = re.sub(r'(?=\\w)\\?|\\?(?=\\w)', ' ? ', text)\n",
        "    text = re.sub(r'(?=\\w)!|!(?=\\w)', ' ! ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(\" bt \", \" but \", text)\n",
        "    text = re.sub(r'(?<=\\w)\\.', '. ', text)\n",
        "    text = re.sub(r'\\((?<=\\w)|(?<=\\w)\\)', '', text)\n",
        "    text = re.sub(\" yo \", \" you \", text)\n",
        "    text = re.sub(\" u \", \" you \", text)\n",
        "    text = re.sub(\" didn't \", \" did not \", text)\n",
        "    text = re.sub(\" can't \", \" can not \", text)\n",
        "    text = re.sub(\" & \", \" and \", text)\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    # this is chatGPT - \"python regex to remove emojis\"\n",
        "    text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]', '', text)\n",
        "    \n",
        "    # my code already above\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n",
        "\n",
        "    # chatGPT - \"remove comma between numbers, i.e. 15,000 -> 15000\"\n",
        "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
        "\n",
        "    # Update the example with the cleaned text\n",
        "    example['clean'] = text.strip()\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PLNEnuVoJqQo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "This is an example of applying the ```clean()``` function you just wrote to a single entry of the dataset. The function added a 'clean' field to the example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ClLlgEPtJqQo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on you…', 'label': 0}\n",
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on you…', 'label': 0, 'clean': 'PERSON nice new signage . are you not concerned by beatlemania style hysterical crowds crongregating on you .'}\n"
          ]
        }
      ],
      "source": [
        "print(tweet_ds['train'][0])\n",
        "print(clean(tweet_ds['train'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5gCMr7svJqQo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's finally use the ```map()``` method and apply your `clean()` function to all entries of the dataset. You can see that the ```clean``` column has been added to each split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jSD3kfxUJqQo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 8993/8993 [00:01<00:00, 7950.06 examples/s]\n",
            "Map: 100%|██████████| 2970/2970 [00:00<00:00, 7729.60 examples/s]\n",
            "Map: 100%|██████████| 999/999 [00:00<00:00, 7533.38 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', 'clean'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', 'clean'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label', 'clean'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.map(clean)\n",
        "print(tweet_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xnPHE9KVJqQo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3. Build Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Q2mkeRlEJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Feature engineering is when NLP task-specific knowledge comes in handy, and make it more likely for a simple classifier to learn the task. This requires us to index tokens and create meaningful representations out of them.\n",
        "\n",
        "Firstm we have to create a vocabulary. In the previous section, we implemeneted the tokenization and the cleaning.\n",
        "\n",
        "In this section we will start by building the vocabulary for your dataset (aka the number of different words in the corpus). We will build it based on the cleaned text of the `train` split. We will investigate some properties of corpora (e.g. Zipf's law, most common words for hate speech language).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "YenDNVQSJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e2'></a>\n",
        "### Exercise 2 (points 2)\n",
        "\n",
        "Fill in the following function so that the ```Counter``` is returned containing all words of a sentence with the number of occurrences. Check the documentation if needed [https://docs.python.org/3/library/collections.html#collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "031VQW14JqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_bag_of_words(sentence):\n",
        "    \"\"\"\n",
        "    Tokenizes the sentence into separate words.\n",
        "    Args:\n",
        "        sentence: string containing a sentence\n",
        "\n",
        "    Returns: Counter of the tokens of the input sentence\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    words = sentence.split()\n",
        "\n",
        "    words_count = Counter(words)\n",
        "\n",
        "    # return your Counter instance\n",
        "    return words_count\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yeNIKx6KJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's test the function you just wrote:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b8Vrbs3sJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERSON nice new signage . are you not concerned by beatlemania style hysterical crowds crongregating on you .\n",
            "Counter({'.': 2, 'you': 2, 'PERSON': 1, 'nice': 1, 'new': 1, 'signage': 1, 'are': 1, 'not': 1, 'concerned': 1, 'by': 1, 'beatlemania': 1, 'style': 1, 'hysterical': 1, 'crowds': 1, 'crongregating': 1, 'on': 1})\n"
          ]
        }
      ],
      "source": [
        "sentence = tweet_ds['train'][0]['clean']\n",
        "print(sentence)\n",
        "\n",
        "sentence_bow = get_bag_of_words(sentence)\n",
        "print(sentence_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IeMnM-WHJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We will also be interested in the word count of the whole dataset (or it's subset). The next function will combine word counts specified as a list into a single Counter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wpRuUYOCJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e3'></a>\n",
        "### Exercise 3 (points 2)\n",
        "\n",
        "Fill in the following function to return a counter combining the provided list of counters. If the word is present in multiple counters the result should sum all the occurrences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Cu5qzL8NJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def aggregate_bags_of_words(bags_of_words):\n",
        "    \"\"\"\n",
        "    Aggregates the provided list containing bags-of-words\n",
        "    Args:\n",
        "        bags_of_words: list of Counters\n",
        "\n",
        "    Returns: Counter of the tokens of all bags-of-words\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    sum_counter = Counter()\n",
        "\n",
        "    for counter in bags_of_words:\n",
        "        sum_counter = sum_counter + counter # you can just sum counters it seems\n",
        "\n",
        "\n",
        "    # return your Counter here\n",
        "    return sum_counter\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cFD0ftRUJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The next function will calculate word counts of each cleaned sentence of the dataset split and then combine them into a single Counter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mChR7aFaJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e4'></a>\n",
        "### Exercise 4 (points 2)\n",
        "\n",
        "Fill in the following function to obtain the counter representing the whole dataset. Use the function ```aggregate_bags_of_words()``` you implemented in the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H8P_tkpHJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_dataset_word_count(dataset_split):\n",
        "    \"\"\"\n",
        "    Creates a word count (a Counter) for the dataset split.\n",
        "    Args:\n",
        "        dataset_split: a dataset split\n",
        "\n",
        "    Returns: Counter of the tokens of the whole split\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    sum_count = Counter()\n",
        "\n",
        "    for i in dataset_split['clean']:\n",
        "        counter = Counter(i.split())\n",
        "        sum_count = sum_count + counter\n",
        "\n",
        "    # return your Counter here\n",
        "    return sum_count\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vXQikOKLJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Here are the word counts of the `train` and `validation` subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nQRv43qGJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train_word_count = get_dataset_word_count(tweet_ds['train'])\n",
        "validation_word_count = get_dataset_word_count(tweet_ds['validation'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3dnzdCd6JqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let us explore them a bit more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JwXQOvRoJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train word count: 18867\n",
            "Most common words:\n",
            "[('.', 7690), ('HASHTAG', 6127), ('PERSON', 6022), ('the', 5464), ('to', 4778), (',', 4489), ('you', 4068), ('a', 3912)]\n",
            "Least common words:\n",
            "[('gangbang', 1), ('(stolen', 1), ('leaf', 1), ('.shame', 1), ('happiest', 1), ('birthdays', 1), ('oooohhhh', 1), ('kennedys', 1)]\n",
            "validation word count: 5125\n",
            "[('.', 1100), ('PERSON', 899), ('the', 749), ('you', 630), ('HASHTAG', 628), ('to', 613), (',', 612), ('a', 497), ('and', 470), ('of', 350)]\n",
            "Most common words:\n",
            "[('.', 1100), ('PERSON', 899), ('the', 749), ('you', 630), ('HASHTAG', 628), ('to', 613), (',', 612), ('a', 497)]\n",
            "Least common words:\n",
            "[('learned', 1), ('independence', 1), ('infeuenc', 1), ('scale', 1), ('hawkers', 1), ('menace', 1), ('constructions', 1), ('appeasement', 1)]\n"
          ]
        }
      ],
      "source": [
        "print(f'train word count: {len(train_word_count)}')\n",
        "print('Most common words:')\n",
        "print(train_word_count.most_common(8))\n",
        "\n",
        "print('Least common words:')\n",
        "print(train_word_count.most_common()[-8:])\n",
        "\n",
        "print(f'validation word count: {len(validation_word_count)}')\n",
        "print(validation_word_count.most_common(10))\n",
        "\n",
        "print('Most common words:')\n",
        "print(validation_word_count.most_common(8))\n",
        "\n",
        "print('Least common words:')\n",
        "print(validation_word_count.most_common()[-8:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "S6hfCe7EJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can also plot the counts of the words. You can check the [Power law](https://en.wikipedia.org/wiki/Power_law) if you are more interested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "yes this is zipf law"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "29gZr1jvJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x12c396b80>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.loglog([val for word,val in train_word_count.most_common()])\n",
        "plt.loglog([val for word,val in validation_word_count.most_common()])\n",
        "plt.xlabel('rank')\n",
        "plt.ylabel('count')\n",
        "plt.legend(['training set','validation set'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "X-ConZy-JqQq",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now let's look at the sentences that are marked as hate and non-hate. We can use the method ```filter``` to get two datasets containing only the sentences labeled the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5XhwCJaIJqQq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 8993/8993 [00:00<00:00, 349625.77 examples/s]\n",
            "Filter: 100%|██████████| 8993/8993 [00:00<00:00, 368159.13 examples/s]\n"
          ]
        }
      ],
      "source": [
        "non_hate_ds = tweet_ds['train'].filter(lambda ex: ex['label'] == 0)\n",
        "hate_ds = tweet_ds['train'].filter(lambda ex: ex['label'] == 1)\n",
        "\n",
        "non_hate_word_count = get_dataset_word_count(non_hate_ds)\n",
        "hate_word_count = get_dataset_word_count(hate_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "hate_words__ = hate_word_count - non_hate_word_count\n",
        "non_hate_words__ = non_hate_word_count - hate_word_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "if you run the below cells, you see which words are in what"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'fucked': 9,\n",
              "         'multiple': 1,\n",
              "         'dick': 21,\n",
              "         'compliment': 3,\n",
              "         'hit': 2,\n",
              "         'talk': 11,\n",
              "         'do': 91,\n",
              "         'they': 86,\n",
              "         'gouged': 1,\n",
              "         'rapefugee': 15,\n",
              "         'girlfriend': 11,\n",
              "         'lookin': 1,\n",
              "         'groupie': 2,\n",
              "         'bitch!': 11,\n",
              "         'lieing': 3,\n",
              "         'sack': 3,\n",
              "         'shit': 26,\n",
              "         '!': 164,\n",
              "         'done': 5,\n",
              "         'why': 40,\n",
              "         'dumped': 2,\n",
              "         'ass': 122,\n",
              "         'bitch': 600,\n",
              "         'send': 19,\n",
              "         'need': 12,\n",
              "         'peter': 2,\n",
              "         'dutton': 3,\n",
              "         'country': 70,\n",
              "         'should': 19,\n",
              "         'overstayers': 1,\n",
              "         'go': 9,\n",
              "         'them': 43,\n",
              "         'cory': 2,\n",
              "         'booker': 12,\n",
              "         'kamala': 6,\n",
              "         'harris': 8,\n",
              "         'competing': 4,\n",
              "         'kavanaugh': 6,\n",
              "         'hearings': 4,\n",
              "         'coulter': 1,\n",
              "         'hilariously': 1,\n",
              "         'yes': 1,\n",
              "         'liberals': 3,\n",
              "         'immediately': 1,\n",
              "         'got': 9,\n",
              "         'triggered': 2,\n",
              "         'her': 69,\n",
              "         'we': 3,\n",
              "         'suck': 45,\n",
              "         'snowflakes': 2,\n",
              "         'let': 33,\n",
              "         'own': 7,\n",
              "         'murderers': 3,\n",
              "         'royal': 1,\n",
              "         'import': 3,\n",
              "         'terrorists': 10,\n",
              "         'HASHTAG': 1319,\n",
              "         'temporarily': 1,\n",
              "         'inactive': 1,\n",
              "         'due': 2,\n",
              "         'shocking': 3,\n",
              "         'listen': 2,\n",
              "         'lil': 22,\n",
              "         'b': 6,\n",
              "         'would': 17,\n",
              "         'fuck': 84,\n",
              "         'dej': 1,\n",
              "         'loaf': 1,\n",
              "         'she': 61,\n",
              "         'prob': 1,\n",
              "         'poison': 2,\n",
              "         'nail': 1,\n",
              "         'polish': 5,\n",
              "         'holes': 13,\n",
              "         'keep': 15,\n",
              "         'whore': 127,\n",
              "         'grown': 1,\n",
              "         'glad': 8,\n",
              "         'another': 2,\n",
              "         'nice': 2,\n",
              "         'illegal': 249,\n",
              "         'border': 26,\n",
              "         'crossings': 1,\n",
              "         'oportunistic': 1,\n",
              "         'entries': 2,\n",
              "         'tax': 7,\n",
              "         'payer': 4,\n",
              "         'funded': 2,\n",
              "         'hotel': 4,\n",
              "         'rooms': 1,\n",
              "         'without': 10,\n",
              "         'strategic': 1,\n",
              "         'unchecked': 2,\n",
              "         'population&amp;migration': 1,\n",
              "         'cities': 3,\n",
              "         'evil': 5,\n",
              "         'dirty': 19,\n",
              "         'india': 6,\n",
              "         '.disincentive': 1,\n",
              "         'such': 2,\n",
              "         'withdrawl': 1,\n",
              "         'sops': 1,\n",
              "         'alien': 61,\n",
              "         'dragged': 1,\n",
              "         'blocks': 3,\n",
              "         'dui': 1,\n",
              "         'knuckle': 1,\n",
              "         'will': 18,\n",
              "         'mines': 1,\n",
              "         'pussy': 47,\n",
              "         'petty': 4,\n",
              "         'wasting': 2,\n",
              "         'seasonal': 2,\n",
              "         'get': 36,\n",
              "         'fuckin': 8,\n",
              "         \"won't\": 5,\n",
              "         'name': 6,\n",
              "         'shameless': 1,\n",
              "         'omg': 8,\n",
              "         'no': 71,\n",
              "         'cunt': 51,\n",
              "         'girl': 18,\n",
              "         'fucktard': 1,\n",
              "         'must': 34,\n",
              "         'stays': 2,\n",
              "         'felon': 5,\n",
              "         'pass': 4,\n",
              "         'mandatory': 5,\n",
              "         'e': 3,\n",
              "         'verify': 4,\n",
              "         'welfare': 23,\n",
              "         'europe': 51,\n",
              "         'kills': 3,\n",
              "         'our': 27,\n",
              "         'culture': 14,\n",
              "         'languages': 1,\n",
              "         'stretches': 1,\n",
              "         'hospitals': 4,\n",
              "         'schools': 6,\n",
              "         'now': 28,\n",
              "         'back': 16,\n",
              "         'please': 7,\n",
              "         'islamic': 9,\n",
              "         'uhhhhhh': 1,\n",
              "         'that’s': 3,\n",
              "         'stupid': 57,\n",
              "         'ma’am': 1,\n",
              "         'because': 6,\n",
              "         'cross': 1,\n",
              "         'doctors': 1,\n",
              "         'want': 76,\n",
              "         'hire': 3,\n",
              "         'ignorant': 7,\n",
              "         'beliefs': 1,\n",
              "         'enough': 11,\n",
              "         'practitioners#': 1,\n",
              "         'betray': 3,\n",
              "         'reject': 4,\n",
              "         'break': 6,\n",
              "         'constitutional': 4,\n",
              "         'laws': 25,\n",
              "         'deport': 57,\n",
              "         'illegals': 113,\n",
              "         'immigr': 1,\n",
              "         'warren': 1,\n",
              "         'radical': 4,\n",
              "         'abolish': 6,\n",
              "         'create': 2,\n",
              "         'open': 17,\n",
              "         'borders': 34,\n",
              "         'be': 19,\n",
              "         'geoff': 1,\n",
              "         'diehl': 1,\n",
              "         'fighting': 5,\n",
              "         'vote': 27,\n",
              "         'senate': 2,\n",
              "         'ma': 1,\n",
              "         'absolute': 2,\n",
              "         'cock': 19,\n",
              "         'womble': 1,\n",
              "         'alude': 1,\n",
              "         'recieved': 1,\n",
              "         'free': 16,\n",
              "         'aswel': 1,\n",
              "         'dan': 4,\n",
              "         'too': 15,\n",
              "         'lying': 8,\n",
              "         'deceitful': 1,\n",
              "         'corrupt': 1,\n",
              "         'losers': 3,\n",
              "         'dems': 13,\n",
              "         '(&': 1,\n",
              "         'disgusting': 8,\n",
              "         'embarrassing': 1,\n",
              "         'pay': 15,\n",
              "         'these': 53,\n",
              "         'salaries': 2,\n",
              "         'democracy': 1,\n",
              "         '.HASHTAG': 42,\n",
              "         '.really': 1,\n",
              "         'cultures': 4,\n",
              "         'bet': 5,\n",
              "         'older': 6,\n",
              "         'save': 2,\n",
              "         'flood': 1,\n",
              "         'immigrants': 28,\n",
              "         'coming': 16,\n",
              "         'irks': 1,\n",
              "         'bitches': 68,\n",
              "         'end': 6,\n",
              "         'complain': 5,\n",
              "         'douche': 5,\n",
              "         'bag': 2,\n",
              "         'foment': 1,\n",
              "         'anger': 3,\n",
              "         'idiots': 5,\n",
              "         'assimilate': 6,\n",
              "         'majority': 2,\n",
              "         'black': 15,\n",
              "         'africans': 8,\n",
              "         'into': 6,\n",
              "         'money': 16,\n",
              "         'megynkelly': 1,\n",
              "         'skank': 28,\n",
              "         '$$': 4,\n",
              "         '.PERSON': 25,\n",
              "         'anymore': 6,\n",
              "         't': 1,\n",
              "         \"you're_\": 1,\n",
              "         'you_': 1,\n",
              "         'life_': 1,\n",
              "         'retarded_': 1,\n",
              "         \"don't\": 1,\n",
              "         'hate': 4,\n",
              "         'girls': 36,\n",
              "         'daddy': 6,\n",
              "         'attention': 6,\n",
              "         'interfere': 1,\n",
              "         'hoe': 148,\n",
              "         'wow!': 2,\n",
              "         'juggle': 1,\n",
              "         'big': 16,\n",
              "         'screw': 1,\n",
              "         'beliefs!': 1,\n",
              "         'packing!': 1,\n",
              "         'nation': 12,\n",
              "         'sloppy': 3,\n",
              "         'queen': 4,\n",
              "         'sucking': 11,\n",
              "         'africa': 2,\n",
              "         'asia': 3,\n",
              "         'asians': 3,\n",
              "         'white': 28,\n",
              "         'genocide': 4,\n",
              "         'basic': 1,\n",
              "         'truths:': 2,\n",
              "         'righthttps://t': 4,\n",
              "         '.co/c0ghvd6imbremigration': 2,\n",
              "         '2018:': 1,\n",
              "         'come': 43,\n",
              "         'go!': 2,\n",
              "         'dumb': 30,\n",
              "         'trick': 3,\n",
              "         '12': 1,\n",
              "         'virgins': 1,\n",
              "         'fucking': 69,\n",
              "         'kys': 1,\n",
              "         'andrew:': 1,\n",
              "         'its': 2,\n",
              "         '.denying': 2,\n",
              "         'canadians': 4,\n",
              "         'racial': 2,\n",
              "         \"'racial\": 1,\n",
              "         \"hatred'\": 1,\n",
              "         'crime:': 1,\n",
              "         'peopl': 1,\n",
              "         'aliens': 61,\n",
              "         'maybe': 6,\n",
              "         'libtarded': 1,\n",
              "         'split': 1,\n",
              "         'take': 18,\n",
              "         'precautions': 1,\n",
              "         'trouble': 1,\n",
              "         'future': 1,\n",
              "         'threat': 1,\n",
              "         \"//'we\": 1,\n",
              "         'tolerate': 1,\n",
              "         'bengal': 3,\n",
              "         \".'with\": 1,\n",
              "         'statements': 1,\n",
              "         'holleringgggggg': 1,\n",
              "         'baby': 9,\n",
              "         'attractive': 1,\n",
              "         'sock': 1,\n",
              "         'hard': 11,\n",
              "         'much': 1,\n",
              "         'mccains': 1,\n",
              "         'funeral': 3,\n",
              "         'costing': 4,\n",
              "         'payers': 1,\n",
              "         'scrap': 1,\n",
              "         'obamacare': 2,\n",
              "         'wall': 54,\n",
              "         'HASHTAGHASHTAG': 54,\n",
              "         'smarter': 4,\n",
              "         'retard': 3,\n",
              "         'toungue': 1,\n",
              "         'l': 2,\n",
              "         'rothsey': 1,\n",
              "         'swop': 2,\n",
              "         'resident': 1,\n",
              "         'until': 7,\n",
              "         'convicted': 2,\n",
              "         'shoplifting': 2,\n",
              "         'deported': 12,\n",
              "         'came': 4,\n",
              "         'illegally': 35,\n",
              "         's': 6,\n",
              "         'taxpayers': 22,\n",
              "         'bills': 4,\n",
              "         'france': 5,\n",
              "         'patch': 1,\n",
              "         'row': 2,\n",
              "         'draw': 4,\n",
              "         'papal': 1,\n",
              "         'rebuke': 1,\n",
              "         'build': 33,\n",
              "         '.we': 5,\n",
              "         'invaded': 5,\n",
              "         'foreign': 8,\n",
              "         'country;': 2,\n",
              "         '#': 7,\n",
              "         'job': 6,\n",
              "         'commander': 1,\n",
              "         'chief': 1,\n",
              "         '.please': 2,\n",
              "         'legs': 1,\n",
              "         'magically': 1,\n",
              "         'broke': 11,\n",
              "         'partying': 1,\n",
              "         'ovaries': 2,\n",
              "         'die': 1,\n",
              "         'hell': 16,\n",
              "         'ugly': 24,\n",
              "         'built!': 1,\n",
              "         'ice:': 1,\n",
              "         '124': 1,\n",
              "         '138': 1,\n",
              "         'comtemplating': 1,\n",
              "         'stfu': 23,\n",
              "         'corner': 3,\n",
              "         'catch': 4,\n",
              "         'communism;': 1,\n",
              "         'roomies': 1,\n",
              "         'bring': 6,\n",
              "         'can’t': 5,\n",
              "         'afford': 4,\n",
              "         \"'come\": 1,\n",
              "         'somethin': 2,\n",
              "         'beggin': 1,\n",
              "         \"bitch'\": 3,\n",
              "         'garcia\"': 1,\n",
              "         'eliminate': 2,\n",
              "         'us/mexico': 1,\n",
              "         'agent': 2,\n",
              "         'enters': 1,\n",
              "         'sabotages': 1,\n",
              "         'american': 25,\n",
              "         'socialist': 1,\n",
              "         'allowing': 5,\n",
              "         'kill': 17,\n",
              "         'breaking': 4,\n",
              "         'lie': 3,\n",
              "         'body': 3,\n",
              "         'dawgg': 1,\n",
              "         'im': 2,\n",
              "         'fr': 3,\n",
              "         'lmfoaoo': 1,\n",
              "         'ian': 2,\n",
              "         'slut': 20,\n",
              "         'fox': 1,\n",
              "         'host:': 1,\n",
              "         'lock': 2,\n",
              "         'clinton': 2,\n",
              "         'pirro': 1,\n",
              "         'sorry': 10,\n",
              "         'sweetie': 2,\n",
              "         'causes': 1,\n",
              "         'problems': 2,\n",
              "         'toothbrushes': 1,\n",
              "         'stop': 76,\n",
              "         'pants': 4,\n",
              "         'above': 2,\n",
              "         'button': 1,\n",
              "         'look': 7,\n",
              "         'croatia': 2,\n",
              "         'ðÿ‡\\xadðÿ‡·:': 2,\n",
              "         'neighborhood': 1,\n",
              "         'zapreå¡iä‡': 1,\n",
              "         'paris': 1,\n",
              "         'tentrefugees': 1,\n",
              "         'parque': 1,\n",
              "         'bois': 1,\n",
              "         'bologne\"': 1,\n",
              "         'foch': 1,\n",
              "         'alabama:': 1,\n",
              "         'behead': 1,\n",
              "         'girlHASHTAG': 1,\n",
              "         'didnt': 5,\n",
              "         'prices': 2,\n",
              "         'widens': 1,\n",
              "         'wealth': 4,\n",
              "         'gaps': 2,\n",
              "         'kids’': 1,\n",
              "         'americans': 33,\n",
              "         'away': 2,\n",
              "         'jobs': 2,\n",
              "         'aggressive': 7,\n",
              "         'whores': 18,\n",
              "         'harder': 2,\n",
              "         'uncomf': 1,\n",
              "         'totally': 2,\n",
              "         'control': 5,\n",
              "         'reigned': 1,\n",
              "         'opportunist': 3,\n",
              "         'millions': 9,\n",
              "         'stuff': 5,\n",
              "         'genie': 1,\n",
              "         'bottle': 2,\n",
              "         'wake': 2,\n",
              "         'tracks!': 1,\n",
              "         'yeen': 1,\n",
              "         'swarm': 1,\n",
              "         'rampant': 1,\n",
              "         'oh': 1,\n",
              "         'ne': 3,\n",
              "         'every': 16,\n",
              "         'chelsea': 2,\n",
              "         'handler': 1,\n",
              "         'tweeting': 4,\n",
              "         'baguettes': 1,\n",
              "         'reduced': 3,\n",
              "         'traditional': 1,\n",
              "         'foods': 1,\n",
              "         'balti': 1,\n",
              "         'always': 21,\n",
              "         'rat': 1,\n",
              "         'againif': 1,\n",
              "         'lets': 3,\n",
              "         'valencia': 1,\n",
              "         'killing': 9,\n",
              "         'islamists': 2,\n",
              "         'shut': 55,\n",
              "         'hes': 1,\n",
              "         'gonna': 3,\n",
              "         'dope': 2,\n",
              "         'steroline': 1,\n",
              "         'edit': 1,\n",
              "         'olly': 1,\n",
              "         'low': 4,\n",
              "         'scum': 4,\n",
              "         'deprave': 1,\n",
              "         'maggots': 1,\n",
              "         'brainless': 2,\n",
              "         'wow': 4,\n",
              "         'allowed': 21,\n",
              "         'happen': 7,\n",
              "         'kidnapper': 1,\n",
              "         'molester': 1,\n",
              "         'needs': 5,\n",
              "         'gone!': 1,\n",
              "         'cong': 3,\n",
              "         'culprit': 1,\n",
              "         'influx': 2,\n",
              "         'both': 8,\n",
              "         'bangladeshis&amp;': 1,\n",
              "         'rhongyias': 1,\n",
              "         'few': 5,\n",
              "         'tenures&amp;': 1,\n",
              "         'encouraged': 1,\n",
              "         'muslim': 20,\n",
              "         'migrants&amp;shooed': 1,\n",
              "         'america': 23,\n",
              "         'good;': 1,\n",
              "         'disavows': 1,\n",
              "         'bad': 3,\n",
              "         'trashy': 4,\n",
              "         'digger': 1,\n",
              "         'makes': 3,\n",
              "         'shes': 5,\n",
              "         'ughhh': 1,\n",
              "         'pos': 2,\n",
              "         'forget': 3,\n",
              "         'feminism': 2,\n",
              "         'answer:': 2,\n",
              "         'scroungers/soldiers': 1,\n",
              "         'caliphate': 3,\n",
              "         'chaos': 1,\n",
              "         'girlfriends/wives/kids': 1,\n",
              "         'refugees!': 1,\n",
              "         'bugger': 1,\n",
              "         'off!': 2,\n",
              "         'bye': 9,\n",
              "         'bey': 2,\n",
              "         'hungary': 4,\n",
              "         'again:prime': 1,\n",
              "         'minster': 1,\n",
              "         'orbã¡n': 1,\n",
              "         'allow': 11,\n",
              "         'incompatible': 2,\n",
              "         'jihadi': 1,\n",
              "         'invaders': 15,\n",
              "         'warned:': 1,\n",
              "         '“europe': 1,\n",
              "         'under': 1,\n",
              "         'invasion': 21,\n",
              "         'lost”': 1,\n",
              "         'blacks': 4,\n",
              "         'europeans': 5,\n",
              "         'underway': 1,\n",
              "         'suggest': 2,\n",
              "         'considered': 1,\n",
              "         'trumped': 2,\n",
              "         'begun': 1,\n",
              "         'placeHASHTAG': 1,\n",
              "         'u': 1,\n",
              "         'helllo': 1,\n",
              "         'tf': 10,\n",
              "         'killed': 9,\n",
              "         'kate': 4,\n",
              "         'steinle': 4,\n",
              "         'sentenced': 1,\n",
              "         'mom': 9,\n",
              "         'learned': 3,\n",
              "         'awaiting': 1,\n",
              "         'producing': 1,\n",
              "         'settle': 2,\n",
              "         'down': 4,\n",
              "         '75': 3,\n",
              "         'turks': 5,\n",
              "         'invade': 1,\n",
              "         'kebabs': 1,\n",
              "         'cameron': 2,\n",
              "         'ur': 29,\n",
              "         'nappy': 1,\n",
              "         'constipated': 1,\n",
              "         'literally': 1,\n",
              "         'fool': 1,\n",
              "         'lawlessness': 1,\n",
              "         'turn': 7,\n",
              "         'atl': 4,\n",
              "         '3rd': 16,\n",
              "         'idiot!': 2,\n",
              "         'watch:': 3,\n",
              "         'fire': 1,\n",
              "         'fun\"': 2,\n",
              "         'speech': 14,\n",
              "         'electrify': 1,\n",
              "         'water': 4,\n",
              "         'rock': 2,\n",
              "         'class': 5,\n",
              "         'skank🤡': 1,\n",
              "         'dreamer': 4,\n",
              "         'aliensto': 1,\n",
              "         'defraud': 1,\n",
              "         'dreamHASHTAG': 1,\n",
              "         'forhis': 1,\n",
              "         'rnc': 1,\n",
              "         'newHASHTAG': 1,\n",
              "         'obamais': 1,\n",
              "         'empty': 4,\n",
              "         '.clint': 1,\n",
              "         'youre': 4,\n",
              "         'doyoung': 1,\n",
              "         'wouldnt': 1,\n",
              "         'treat': 1,\n",
              "         'hillary': 12,\n",
              "         'lied': 8,\n",
              "         'keeping': 5,\n",
              "         'greedy': 4,\n",
              "         'ðÿ‡ºðÿ‡¸': 3,\n",
              "         'preying': 2,\n",
              "         'tendency': 1,\n",
              "         '‼️': 2,\n",
              "         'destroy': 15,\n",
              "         'means': 5,\n",
              "         'streetfights': 1,\n",
              "         'streets': 19,\n",
              "         'filthy': 8,\n",
              "         'idgaf': 1,\n",
              "         'pregnant': 8,\n",
              "         'ill': 10,\n",
              "         'cops': 4,\n",
              "         'countries!': 1,\n",
              "         'option': 1,\n",
              "         'appears': 2,\n",
              "         'crazy': 12,\n",
              "         'already': 5,\n",
              "         'mad': 15,\n",
              "         'cuz': 6,\n",
              "         'nutted': 1,\n",
              "         'quick': 8,\n",
              "         'dollars': 7,\n",
              "         'side': 6,\n",
              "         'lower': 6,\n",
              "         'taxes': 4,\n",
              "         'tfsa': 1,\n",
              "         'policysome': 1,\n",
              "         '.co/dimpwggsvz': 1,\n",
              "         '(h': 1,\n",
              "         '.kickl': 1,\n",
              "         'comunistic': 1,\n",
              "         'everything': 7,\n",
              "         'austria': 6,\n",
              "         'disturb': 1,\n",
              "         'beware': 4,\n",
              "         'usa': 35,\n",
              "         'illigal': 12,\n",
              "         'anyway': 3,\n",
              "         'bho': 1,\n",
              "         'holder': 1,\n",
              "         'throw': 9,\n",
              "         'glitter': 1,\n",
              "         'pasture!': 1,\n",
              "         'dig': 3,\n",
              "         \"'oh\": 3,\n",
              "         'elbowed': 1,\n",
              "         'triple': 2,\n",
              "         'chinned': 1,\n",
              "         'nails': 1,\n",
              "         'ont': 1,\n",
              "         'smelly': 2,\n",
              "         'fart': 1,\n",
              "         'before': 23,\n",
              "         'neighbour': 2,\n",
              "         'preferably': 1,\n",
              "         'tortures': 1,\n",
              "         'excuse': 1,\n",
              "         'criminals': 25,\n",
              "         'immigrants!': 1,\n",
              "         'legally': 12,\n",
              "         'amnesty': 7,\n",
              "         'lmaoo': 3,\n",
              "         'those': 10,\n",
              "         '?!': 9,\n",
              "         'wall!': 8,\n",
              "         'it!': 9,\n",
              "         '62': 1,\n",
              "         'voted': 10,\n",
              "         'promices': 1,\n",
              "         'starlets': 1,\n",
              "         'worse': 1,\n",
              "         'minds': 2,\n",
              "         'around!': 1,\n",
              "         'menace': 1,\n",
              "         'growth': 1,\n",
              "         'population&amp;illegal': 1,\n",
              "         'w': 13,\n",
              "         'crime/returning': 1,\n",
              "         'security': 1,\n",
              "         'ignore': 3,\n",
              "         'engineers/cultural': 1,\n",
              "         'marxists': 1,\n",
              "         'pimps': 1,\n",
              "         'put': 11,\n",
              "         'penis': 2,\n",
              "         'digging': 1,\n",
              "         'hilla': 1,\n",
              "         'offend': 3,\n",
              "         'femalesss': 1,\n",
              "         'bitchslut': 1,\n",
              "         'feministshit': 1,\n",
              "         'belong': 4,\n",
              "         'dying': 1,\n",
              "         'everyday': 3,\n",
              "         'begging': 1,\n",
              "         'godless': 1,\n",
              "         'wond': 1,\n",
              "         'babe': 2,\n",
              "         'insane': 5,\n",
              "         'anal': 3,\n",
              "         'auditions': 1,\n",
              "         'fav': 1,\n",
              "         'experiment': 1,\n",
              "         'multiculturalism': 3,\n",
              "         'amounts': 1,\n",
              "         '.s': 2,\n",
              "         'yah': 1,\n",
              "         'so!': 1,\n",
              "         'delhi': 16,\n",
              "         'understood': 1,\n",
              "         'gameplan': 1,\n",
              "         'focussing': 1,\n",
              "         'votebank': 1,\n",
              "         'encroachments': 4,\n",
              "         'filth': 5,\n",
              "         'areas': 2,\n",
              "         '.it': 6,\n",
              "         'lesbian': 5,\n",
              "         'trending': 1,\n",
              "         'checked': 2,\n",
              "         'HASHTAG!': 15,\n",
              "         'trending!': 1,\n",
              "         'mouth': 13,\n",
              "         'cocks': 1,\n",
              "         'cum': 4,\n",
              "         'helpless': 2,\n",
              "         'choke': 5,\n",
              "         'cum!': 1,\n",
              "         'exhauste': 1,\n",
              "         'waiting': 3,\n",
              "         'awol': 1,\n",
              "         'scared': 6,\n",
              "         'hello': 5,\n",
              "         'kitty': 2,\n",
              "         'even': 11,\n",
              "         'existed': 1,\n",
              "         'enforce': 3,\n",
              "         'laws!': 1,\n",
              "         'homeless': 11,\n",
              "         'lots': 4,\n",
              "         'veterans': 2,\n",
              "         'lend': 1,\n",
              "         'well!': 4,\n",
              "         'regarding': 2,\n",
              "         'disaster': 6,\n",
              "         '.erdogan': 1,\n",
              "         'flooded': 1,\n",
              "         'male': 2,\n",
              "         'helped': 2,\n",
              "         'destabilize': 1,\n",
              "         'supported': 3,\n",
              "         'extremists': 1,\n",
              "         'tired': 4,\n",
              "         '4': 2,\n",
              "         'ppl': 2,\n",
              "         'deporting': 2,\n",
              "         'foreigns': 1,\n",
              "         'medicare': 1,\n",
              "         'medicaid': 2,\n",
              "         'strained': 1,\n",
              "         'benefits\"': 1,\n",
              "         'quo': 1,\n",
              "         'entry': 4,\n",
              "         'ms': 11,\n",
              "         'states': 6,\n",
              "         'daca': 7,\n",
              "         'recipients': 3,\n",
              "         '207': 1,\n",
              "         'murdersPERSON': 1,\n",
              "         'HASHTAGHASHTAGHASHTAG': 17,\n",
              "         'fucker': 4,\n",
              "         'rode': 1,\n",
              "         'wet': 3,\n",
              "         'muscular': 1,\n",
              "         'olympian!': 1,\n",
              "         'gang': 13,\n",
              "         'embedding': 1,\n",
              "         'units': 1,\n",
              "         'pouring': 1,\n",
              "         'grande': 1,\n",
              "         'modern': 2,\n",
              "         '‘˜savage': 1,\n",
              "         'indians’': 1,\n",
              "         \"shouldn't\": 9,\n",
              "         'step': 1,\n",
              "         'qualifying': 1,\n",
              "         '(including': 1,\n",
              "         'wc': 1,\n",
              "         'africans/whites': 1,\n",
              "         'cheering': 1,\n",
              "         'guinea': 1,\n",
              "         'crazy!': 2,\n",
              "         'dogs': 5,\n",
              "         'pursuit': 2,\n",
              "         'arguing': 1,\n",
              "         'men/women': 1,\n",
              "         'system': 1,\n",
              "         'prevailing': 1,\n",
              "         'ru': 1,\n",
              "         'shoved': 1,\n",
              "         'primetime': 1,\n",
              "         'night:': 1,\n",
              "         'muslims': 10,\n",
              "         'assam': 10,\n",
              "         '1971': 2,\n",
              "         '1991': 1,\n",
              "         '77': 3,\n",
              "         'per': 6,\n",
              "         'cent': 1,\n",
              "         'indigenous': 1,\n",
              "         'bangladeshis': 3,\n",
              "         '.bangladeshi': 3,\n",
              "         'ewk': 1,\n",
              "         'wearing': 4,\n",
              "         'wack': 2,\n",
              "         'anhyone': 1,\n",
              "         'smell': 1,\n",
              "         'bitches!': 2,\n",
              "         'ejeculated': 1,\n",
              "         'whore!': 10,\n",
              "         'smh': 5,\n",
              "         'seats': 2,\n",
              "         'eh': 3,\n",
              "         '?:gang': 1,\n",
              "         'caused': 1,\n",
              "         'congestion': 1,\n",
              "         'overpopulation': 6,\n",
              "         \"'di\": 1,\n",
              "         'woke': 2,\n",
              "         'message': 1,\n",
              "         '“wtf': 1,\n",
              "         '?”': 1,\n",
              "         'nooo!': 1,\n",
              "         'she’s': 11,\n",
              "         'nose': 4,\n",
              "         'looks': 2,\n",
              "         'cute': 1,\n",
              "         'girlfriends': 1,\n",
              "         'leave': 18,\n",
              "         'theyre': 2,\n",
              "         'tie': 1,\n",
              "         'fvcking': 2,\n",
              "         'apologize!': 1,\n",
              "         'cares': 3,\n",
              "         'susan': 1,\n",
              "         'alota': 1,\n",
              "         'hoes': 49,\n",
              "         'bout': 9,\n",
              "         'dawg': 1,\n",
              "         'careful': 2,\n",
              "         'cracker': 2,\n",
              "         'moms': 3,\n",
              "         'voting': 5,\n",
              "         'personality': 1,\n",
              "         'eyebrows': 3,\n",
              "         'skinny': 6,\n",
              "         'ride': 5,\n",
              "         'aight': 1,\n",
              "         'puppets': 4,\n",
              "         'establishment': 3,\n",
              "         '(500': 1,\n",
              "         '.000': 3,\n",
              "         'stay': 9,\n",
              "         'italia': 4,\n",
              "         'collaps': 1,\n",
              "         'likes': 7,\n",
              "         'naw': 2,\n",
              "         'dicchead': 1,\n",
              "         'centre/states': 1,\n",
              "         'hound': 1,\n",
              "         '&amp;deport': 2,\n",
              "         'fat': 5,\n",
              "         'viral!': 1,\n",
              "         'rt!': 1,\n",
              "         'dump': 2,\n",
              "         'road': 5,\n",
              "         'sucked': 4,\n",
              "         'fame': 6,\n",
              "         'patriate': 1,\n",
              "         'sanjiv': 1,\n",
              "         'javid': 1,\n",
              "         'lazy': 7,\n",
              "         '.police': 1,\n",
              "         'attacked': 5,\n",
              "         'scarf': 1,\n",
              "         '.person': 1,\n",
              "         'recording': 2,\n",
              "         'funny': 4,\n",
              "         'laughs': 1,\n",
              "         'w/a': 2,\n",
              "         'wound': 1,\n",
              "         '.co/4w471ufy6f': 1,\n",
              "         'lol': 6,\n",
              "         'kid': 2,\n",
              "         'benefits': 3,\n",
              "         'wife': 12,\n",
              "         'votes': 2,\n",
              "         'simple': 2,\n",
              "         'combined': 1,\n",
              "         'unless': 6,\n",
              "         'arrival': 2,\n",
              "         'soft': 3,\n",
              "         'touches': 2,\n",
              "         'ps': 3,\n",
              "         'gulf': 3,\n",
              "         'saudi': 1,\n",
              "         'intake': 1,\n",
              "         \"'refugees'\": 2,\n",
              "         '=': 5,\n",
              "         'sup': 2,\n",
              "         'fuxking': 2,\n",
              "         'ful': 1,\n",
              "         'touching': 2,\n",
              "         'ima': 5,\n",
              "         'nasty': 10,\n",
              "         'sucks': 4,\n",
              "         'victory': 1,\n",
              "         'followed': 2,\n",
              "         'previously': 3,\n",
              "         'ahshshsjs': 1,\n",
              "         'deserves': 4,\n",
              "         'ahsjsjd': 1,\n",
              "         'lmfao': 3,\n",
              "         'biddy': 2,\n",
              "         'sayin': 4,\n",
              "         'resting': 2,\n",
              "         'innocent': 5,\n",
              "         'slaughtered!': 1,\n",
              "         'taxpayer': 10,\n",
              "         'rockelle': 1,\n",
              "         'garza': 1,\n",
              "         'fleece': 1,\n",
              "         'abortions': 1,\n",
              "         'spanked': 2,\n",
              "         'gimmie': 1,\n",
              "         'aeso': 1,\n",
              "         'recession': 1,\n",
              "         'sustainability': 1,\n",
              "         'energy': 1,\n",
              "         'fellas': 1,\n",
              "         'girls/women': 1,\n",
              "         'play': 3,\n",
              "         'refuse': 1,\n",
              "         'hugs!': 1,\n",
              "         'airplane': 1,\n",
              "         'HASHTAG~>': 1,\n",
              "         'worried': 1,\n",
              "         'bringing': 5,\n",
              "         'supposed': 4,\n",
              "         'to!': 2,\n",
              "         'entire': 1,\n",
              "         'preserve': 2,\n",
              "         'islam!': 1,\n",
              "         '#HASHTAG': 3,\n",
              "         'cunt!': 5,\n",
              "         'death!': 2,\n",
              "         'congrats': 2,\n",
              "         ':)': 1,\n",
              "         'loud': 2,\n",
              "         'absurdity!': 1,\n",
              "         'swedes': 2,\n",
              "         'nectar': 1,\n",
              "         'welfare!': 1,\n",
              "         'moved': 1,\n",
              "         'willing': 3,\n",
              "         'fund': 6,\n",
              "         'roachingfugees': 1,\n",
              "         'swedish': 1,\n",
              "         'gormint': 1,\n",
              "         'grand': 1,\n",
              "         'undertaking': 1,\n",
              "         'follow': 8,\n",
              "         'facilities': 1,\n",
              "         'luxurious': 1,\n",
              "         '(aka': 1,\n",
              "         'iq': 4,\n",
              "         'libs:': 1,\n",
              "         'enter': 14,\n",
              "         'yoga': 3,\n",
              "         \"'they're\": 1,\n",
              "         \"comfy!'\": 1,\n",
              "         'ladies': 15,\n",
              "         'yall': 2,\n",
              "         'angelas': 1,\n",
              "         'koreans': 2,\n",
              "         'smart': 2,\n",
              "         'sheme': 1,\n",
              "         'foodbanks': 2,\n",
              "         'respect': 1,\n",
              "         \"wouldn't\": 3,\n",
              "         'himseld': 1,\n",
              "         'exactly': 1,\n",
              "         'sick': 5,\n",
              "         \"isn't\": 2,\n",
              "         'exit': 1,\n",
              "         'refreshing!': 1,\n",
              "         \"'political\": 1,\n",
              "         'correctness\"': 1,\n",
              "         'rapes': 10,\n",
              "         'halifax': 1,\n",
              "         'invades': 1,\n",
              "         'society': 1,\n",
              "         \"money'\": 1,\n",
              "         'latina': 1,\n",
              "         'giant': 1,\n",
              "         'pig': 6,\n",
              "         'bastard': 3,\n",
              "         'wish': 2,\n",
              "         'tits': 3,\n",
              "         'beat': 13,\n",
              "         'snorter': 1,\n",
              "         \"ain't\": 6,\n",
              "         'you’re': 15,\n",
              "         'decriminalisation': 1,\n",
              "         'drugs': 1,\n",
              "         'untrue': 1,\n",
              "         '80%': 1,\n",
              "         'crosss': 1,\n",
              "         'raped': 7,\n",
              "         ...})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hate_words__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'PERSON': 234,\n",
              "         'new': 108,\n",
              "         'signage': 1,\n",
              "         '.': 1340,\n",
              "         'are': 220,\n",
              "         'you': 392,\n",
              "         'not': 75,\n",
              "         'concerned': 5,\n",
              "         'by': 99,\n",
              "         'beatlemania': 1,\n",
              "         'style': 3,\n",
              "         'hysterical': 45,\n",
              "         'crowds': 1,\n",
              "         'crongregating': 1,\n",
              "         'on': 262,\n",
              "         'woman': 49,\n",
              "         'like': 24,\n",
              "         'me': 91,\n",
              "         'flirting': 5,\n",
              "         'so': 30,\n",
              "         'tell': 3,\n",
              "         'about': 162,\n",
              "         'your': 52,\n",
              "         'father': 13,\n",
              "         'the': 932,\n",
              "         'philippine': 6,\n",
              "         'catholic': 19,\n",
              "         \"bishops'\": 3,\n",
              "         'work': 34,\n",
              "         'for': 368,\n",
              "         'migrant': 90,\n",
              "         'workers': 39,\n",
              "         'focus': 11,\n",
              "         'families': 56,\n",
              "         'who': 103,\n",
              "         'great': 5,\n",
              "         'when': 276,\n",
              "         'cuffing': 2,\n",
              "         'season': 14,\n",
              "         'is': 233,\n",
              "         'finally': 10,\n",
              "         'over': 57,\n",
              "         'only': 4,\n",
              "         'that': 118,\n",
              "         'in': 494,\n",
              "         'which': 27,\n",
              "         'can': 64,\n",
              "         'hear': 8,\n",
              "         'what': 61,\n",
              "         \"i'm\": 50,\n",
              "         'saying': 28,\n",
              "         'baba': 1,\n",
              "         'ram': 84,\n",
              "         'dass': 3,\n",
              "         'more': 37,\n",
              "         'migrants': 78,\n",
              "         'sea': 8,\n",
              "         'route': 3,\n",
              "         'to': 640,\n",
              "         'than': 40,\n",
              "         'italy': 27,\n",
              "         'this': 60,\n",
              "         'year:': 2,\n",
              "         'un': 43,\n",
              "         'essential': 5,\n",
              "         'reading': 8,\n",
              "         'scribner': 1,\n",
              "         ',': 901,\n",
              "         'nebraska': 1,\n",
              "         'considering': 4,\n",
              "         'an': 112,\n",
              "         'anti': 63,\n",
              "         'immigrant': 537,\n",
              "         'measure': 3,\n",
              "         'rt': 19,\n",
              "         'i': 187,\n",
              "         'am': 31,\n",
              "         'flattered': 1,\n",
              "         'tbh': 4,\n",
              "         'orgasm': 1,\n",
              "         'just': 62,\n",
              "         'thinking': 8,\n",
              "         'of': 779,\n",
              "         'it': 184,\n",
              "         'really': 29,\n",
              "         'good': 23,\n",
              "         'at': 180,\n",
              "         'making': 9,\n",
              "         'anatomically': 1,\n",
              "         'correct~': 1,\n",
              "         'bland': 1,\n",
              "         'all': 207,\n",
              "         'women': 64,\n",
              "         'tiny': 4,\n",
              "         'and': 460,\n",
              "         'fit': 4,\n",
              "         'men': 247,\n",
              "         'bulky': 1,\n",
              "         'honest': 6,\n",
              "         'article': 18,\n",
              "         'immigration': 70,\n",
              "         'a': 332,\n",
              "         'lefty': 1,\n",
              "         '?': 82,\n",
              "         'pigs': 1,\n",
              "         'flying': 3,\n",
              "         '-': 32,\n",
              "         'miracle': 1,\n",
              "         'spread': 1,\n",
              "         'word': 4,\n",
              "         '-&gt;&gt;': 1,\n",
              "         'using': 26,\n",
              "         'frequent': 2,\n",
              "         'flyer': 1,\n",
              "         'miles': 2,\n",
              "         'reunite': 8,\n",
              "         'families!': 2,\n",
              "         'protest': 19,\n",
              "         \"'irrational'\": 1,\n",
              "         \"'hysterical'\": 9,\n",
              "         \"'bitters\": 1,\n",
              "         \"spinsters'\": 1,\n",
              "         'long': 8,\n",
              "         'history': 14,\n",
              "         'dismi': 1,\n",
              "         'still': 23,\n",
              "         'better': 8,\n",
              "         'federal': 7,\n",
              "         'judge': 18,\n",
              "         'HASHTAG:': 7,\n",
              "         'imminent': 2,\n",
              "         'danger\"': 1,\n",
              "         'injury': 1,\n",
              "         'visa': 3,\n",
              "         'holders\"': 1,\n",
              "         'etc': 17,\n",
              "         'protect': 4,\n",
              "         'refugees': 273,\n",
              "         'from': 175,\n",
              "         '‘˜big': 1,\n",
              "         'ideas’': 1,\n",
              "         'designed': 10,\n",
              "         'kevin': 2,\n",
              "         'rosero': 1,\n",
              "         'latino': 4,\n",
              "         'experience': 14,\n",
              "         'oral': 1,\n",
              "         'project': 7,\n",
              "         'help': 71,\n",
              "         'identifying': 1,\n",
              "         'newark': 1,\n",
              "         'residents': 1,\n",
              "         'interview': 11,\n",
              "         'capture': 1,\n",
              "         'their': 64,\n",
              "         'stories': 17,\n",
              "         'experiences': 8,\n",
              "         'info:': 1,\n",
              "         'tonight': 14,\n",
              "         'some': 14,\n",
              "         'people': 59,\n",
              "         'understand': 17,\n",
              "         'injustices': 1,\n",
              "         'happening': 1,\n",
              "         'trust': 6,\n",
              "         'getting': 7,\n",
              "         'my': 34,\n",
              "         'panties': 2,\n",
              "         'wad': 1,\n",
              "         'things': 15,\n",
              "         'fact': 16,\n",
              "         '(minority': 1,\n",
              "         'suffering': 5,\n",
              "         'affects': 3,\n",
              "         'directly': 4,\n",
              "         'have': 115,\n",
              "         'been': 59,\n",
              "         'called': 4,\n",
              "         'son': 19,\n",
              "         'today': 32,\n",
              "         '1': 22,\n",
              "         'yet': 6,\n",
              "         'pope': 6,\n",
              "         'francis:': 1,\n",
              "         'provide': 16,\n",
              "         '‘˜enrichment’': 1,\n",
              "         '‘˜threats’': 1,\n",
              "         'were': 50,\n",
              "         'planning': 2,\n",
              "         'with': 189,\n",
              "         'children': 107,\n",
              "         'kidnapped': 3,\n",
              "         'slavery': 3,\n",
              "         'sex': 17,\n",
              "         'organ': 1,\n",
              "         'harvesting': 1,\n",
              "         'masturbate': 1,\n",
              "         'intensive': 1,\n",
              "         'truth': 8,\n",
              "         'christian': 2,\n",
              "         'victim': 14,\n",
              "         'quarrel': 1,\n",
              "         'between': 20,\n",
              "         'syrian': 44,\n",
              "         'carrying': 6,\n",
              "         '&amp;': 116,\n",
              "         'cleavers': 1,\n",
              "         'lebanon': 14,\n",
              "         'corecivic': 2,\n",
              "         'found': 13,\n",
              "         'best': 19,\n",
              "         'interest': 4,\n",
              "         'grow': 11,\n",
              "         'us': 82,\n",
              "         'detention': 37,\n",
              "         'capacity': 2,\n",
              "         '450%': 1,\n",
              "         'expanding': 2,\n",
              "         'number': 11,\n",
              "         'mother': 7,\n",
              "         'child': 24,\n",
              "         'beds': 2,\n",
              "         '3': 22,\n",
              "         '500': 7,\n",
              "         'up': 7,\n",
              "         '20': 10,\n",
              "         '000': 38,\n",
              "         'signals': 4,\n",
              "         'largest': 7,\n",
              "         'increase': 2,\n",
              "         'since': 25,\n",
              "         'ww2': 1,\n",
              "         'photo:': 1,\n",
              "         'henry': 1,\n",
              "         'wonder': 3,\n",
              "         'where': 27,\n",
              "         'caitlyn': 2,\n",
              "         'gets': 9,\n",
              "         'his': 68,\n",
              "         'services': 19,\n",
              "         'bishop': 2,\n",
              "         'appeals': 3,\n",
              "         'church': 14,\n",
              "         'groups': 28,\n",
              "         'action': 12,\n",
              "         'as': 129,\n",
              "         'account': 9,\n",
              "         '11': 7,\n",
              "         'percent': 6,\n",
              "         'reported': 3,\n",
              "         'cases': 1,\n",
              "         'forcing': 1,\n",
              "         'anyone': 10,\n",
              "         'out': 32,\n",
              "         'view': 7,\n",
              "         'believe': 10,\n",
              "         'or': 69,\n",
              "         'prerogative': 1,\n",
              "         'if': 7,\n",
              "         'person': 19,\n",
              "         'agree': 3,\n",
              "         'thanks': 10,\n",
              "         'fed': 2,\n",
              "         'govt': 11,\n",
              "         'initiate': 1,\n",
              "         'immediate': 2,\n",
              "         '2': 28,\n",
              "         'cancel': 1,\n",
              "         'id': 7,\n",
              "         'cards': 8,\n",
              "         'aadhaar': 1,\n",
              "         'commited\"': 1,\n",
              "         'political': 31,\n",
              "         'any': 32,\n",
              "         'remain': 3,\n",
              "         'firm': 4,\n",
              "         'concerns': 2,\n",
              "         'russia': 23,\n",
              "         'turkey': 17,\n",
              "         'jordan': 17,\n",
              "         'talks': 6,\n",
              "         'return': 31,\n",
              "         '‘“': 11,\n",
              "         'lavrov': 2,\n",
              "         'neverrrrrrrrrrr': 1,\n",
              "         'thank': 39,\n",
              "         'taking': 2,\n",
              "         'stand': 26,\n",
              "         'fight': 6,\n",
              "         'misinformation': 1,\n",
              "         'rights': 54,\n",
              "         'eritrean': 2,\n",
              "         'israel': 7,\n",
              "         'article:https://t': 1,\n",
              "         '.co/0sstuvgq6z': 1,\n",
              "         'orders': 3,\n",
              "         'government': 26,\n",
              "         'release': 5,\n",
              "         'kids': 2,\n",
              "         'texas': 6,\n",
              "         'shelter': 17,\n",
              "         '|': 32,\n",
              "         'tribune': 3,\n",
              "         'dare': 1,\n",
              "         'trump': 95,\n",
              "         'supporters': 4,\n",
              "         'call': 4,\n",
              "         'humane': 1,\n",
              "         'was': 106,\n",
              "         'delighted': 1,\n",
              "         'learn': 25,\n",
              "         'kirk': 1,\n",
              "         'conspiracy': 1,\n",
              "         'theory': 3,\n",
              "         'touting': 1,\n",
              "         'founder': 3,\n",
              "         'executive': 2,\n",
              "         'director': 9,\n",
              "         'nationalist': 1,\n",
              "         'allied': 1,\n",
              "         'turning': 2,\n",
              "         'point': 22,\n",
              "         '(tpusa': 1,\n",
              "         'did': 23,\n",
              "         'very': 15,\n",
              "         'morning': 4,\n",
              "         'either': 12,\n",
              "         'med': 2,\n",
              "         'house/eu': 1,\n",
              "         'adult': 5,\n",
              "         'day': 30,\n",
              "         'care': 4,\n",
              "         'camp': 30,\n",
              "         'seeking': 5,\n",
              "         'handouts': 1,\n",
              "         'profit': 10,\n",
              "         '.tunisia': 1,\n",
              "         'abusing': 4,\n",
              "         'eu': 18,\n",
              "         'has': 82,\n",
              "         'leverage': 1,\n",
              "         'but': 106,\n",
              "         'lacks': 4,\n",
              "         'use': 16,\n",
              "         'messageðÿ‘‰either': 1,\n",
              "         'boats': 4,\n",
              "         'bury': 3,\n",
              "         'economically': 2,\n",
              "         'esp': 5,\n",
              "         'leaders': 16,\n",
              "         'tomorrow': 18,\n",
              "         'including': 11,\n",
              "         'responses': 3,\n",
              "         'world': 57,\n",
              "         'relief': 30,\n",
              "         'continuing': 3,\n",
              "         'legal': 11,\n",
              "         'aid': 24,\n",
              "         'advocating': 2,\n",
              "         'separated': 18,\n",
              "         'make': 27,\n",
              "         'difference': 6,\n",
              "         'âž\\x9d': 48,\n",
              "         'refugee': 185,\n",
              "         'sole': 1,\n",
              "         'survivor': 3,\n",
              "         'how': 78,\n",
              "         'one': 50,\n",
              "         'become': 10,\n",
              "         'shoemaker': 1,\n",
              "         'after': 64,\n",
              "         'night': 14,\n",
              "         'wanna': 6,\n",
              "         'cause': 14,\n",
              "         'guy': 2,\n",
              "         '“oh': 1,\n",
              "         'god': 3,\n",
              "         '?!”': 1,\n",
              "         'monologue': 1,\n",
              "         'hundreds': 18,\n",
              "         'home': 18,\n",
              "         'abc': 1,\n",
              "         'news': 20,\n",
              "         'video:': 7,\n",
              "         'activist': 9,\n",
              "         'climbed': 1,\n",
              "         'statue': 1,\n",
              "         'liberty': 2,\n",
              "         'song:': 1,\n",
              "         'motherfuckers': 1,\n",
              "         'lot': 13,\n",
              "         'therapists': 1,\n",
              "         'meet': 16,\n",
              "         'see': 30,\n",
              "         'true': 17,\n",
              "         \"'you\": 5,\n",
              "         'miss': 11,\n",
              "         'shots': 2,\n",
              "         \".'\": 2,\n",
              "         '--': 4,\n",
              "         'wayne': 1,\n",
              "         'gretzky': 1,\n",
              "         'actually': 6,\n",
              "         'public': 5,\n",
              "         'charge\"': 2,\n",
              "         'law': 8,\n",
              "         'books': 4,\n",
              "         '100': 5,\n",
              "         'years': 24,\n",
              "         'hold': 1,\n",
              "         'green': 8,\n",
              "         'card': 1,\n",
              "         'mexican': 5,\n",
              "         'central': 6,\n",
              "         'alleges': 1,\n",
              "         'gave': 7,\n",
              "         'rakban': 1,\n",
              "         'expired': 3,\n",
              "         'sweets': 1,\n",
              "         'list': 14,\n",
              "         'fails': 3,\n",
              "         \"'50\": 1,\n",
              "         'think': 18,\n",
              "         'doing': 22,\n",
              "         \"beans'\": 1,\n",
              "         'kunt': 33,\n",
              "         'sweden': 5,\n",
              "         'dentist': 1,\n",
              "         'fined': 1,\n",
              "         'nearly': 1,\n",
              "         '$50': 1,\n",
              "         'revealing': 2,\n",
              "         'many': 32,\n",
              "         'via': 59,\n",
              "         'swedenstan': 1,\n",
              "         'update': 9,\n",
              "         'bill': 6,\n",
              "         'endeavour': 1,\n",
              "         'cannot': 7,\n",
              "         'impact': 10,\n",
              "         'current': 9,\n",
              "         'situation': 8,\n",
              "         'part': 23,\n",
              "         '“deal': 1,\n",
              "         'century”': 1,\n",
              "         'aims': 2,\n",
              "         'palestinian': 20,\n",
              "         'refugees’': 6,\n",
              "         'right': 15,\n",
              "         'works': 8,\n",
              "         'hand': 2,\n",
              "         'israeli': 5,\n",
              "         'racist': 19,\n",
              "         'state': 25,\n",
              "         'ugh!': 1,\n",
              "         'christians': 1,\n",
              "         'here': 8,\n",
              "         'trying': 7,\n",
              "         'rights!': 4,\n",
              "         'life': 46,\n",
              "         'then': 12,\n",
              "         'fix': 5,\n",
              "         'broken': 4,\n",
              "         'foster': 2,\n",
              "         'welcome': 15,\n",
              "         'arms': 3,\n",
              "         'follower': 2,\n",
              "         'christ': 4,\n",
              "         'while': 11,\n",
              "         'neighbors': 12,\n",
              "         'yikes': 1,\n",
              "         'grandmother': 2,\n",
              "         'never': 8,\n",
              "         'knew': 2,\n",
              "         '2012': 2,\n",
              "         'ahead': 4,\n",
              "         'could': 31,\n",
              "         'explain': 5,\n",
              "         'praised': 1,\n",
              "         'research': 8,\n",
              "         'foundation': 9,\n",
              "         'linked': 1,\n",
              "         'orban': 12,\n",
              "         'far': 3,\n",
              "         'hosting': 10,\n",
              "         'events': 10,\n",
              "         'hungarian': 3,\n",
              "         'embassy': 3,\n",
              "         'fuckboys': 1,\n",
              "         'little': 7,\n",
              "         'heart': 10,\n",
              "         'iraqi': 16,\n",
              "         'accused': 34,\n",
              "         'critically': 1,\n",
              "         'wounding': 1,\n",
              "         'colorado': 6,\n",
              "         'cop': 11,\n",
              "         'evaded': 3,\n",
              "         'deportation': 8,\n",
              "         'despite': 16,\n",
              "         'record': 5,\n",
              "         'he': 85,\n",
              "         'buckle': 1,\n",
              "         'demands': 3,\n",
              "         'swear': 1,\n",
              "         'undying': 1,\n",
              "         'a*******': 1,\n",
              "         'enemy': 1,\n",
              "         'nation!': 1,\n",
              "         'administration': 34,\n",
              "         'may': 19,\n",
              "         'creating': 3,\n",
              "         \"'permanently\": 2,\n",
              "         \"orphaned'\": 1,\n",
              "         'says': 75,\n",
              "         '–': 12,\n",
              "         'monthly': 3,\n",
              "         'effects': 3,\n",
              "         'third': 2,\n",
              "         'canada': 14,\n",
              "         'heated:': 1,\n",
              "         'breaking:': 1,\n",
              "         'sets': 2,\n",
              "         'man': 45,\n",
              "         'catching': 1,\n",
              "         'him': 42,\n",
              "         'rape': 80,\n",
              "         '7': 12,\n",
              "         'yr': 9,\n",
              "         'old': 19,\n",
              "         'youth': 16,\n",
              "         'worker': 20,\n",
              "         'sexually': 14,\n",
              "         'boys': 24,\n",
              "         'year': 45,\n",
              "         'young': 13,\n",
              "         'mali': 1,\n",
              "         'president': 11,\n",
              "         'awarded': 1,\n",
              "         'national': 6,\n",
              "         'hero': 4,\n",
              "         'medal': 3,\n",
              "         'bravery': 1,\n",
              "         'inviting': 2,\n",
              "         'elysée': 1,\n",
              "         'palace': 1,\n",
              "         'offered': 5,\n",
              "         'french': 2,\n",
              "         'citizenship': 2,\n",
              "         'https://t': 5,\n",
              "         '.co/hkdegti5ya': 1,\n",
              "         'af': 23,\n",
              "         'hahahahahahahaha': 1,\n",
              "         'thick': 3,\n",
              "         'guys': 15,\n",
              "         'had': 5,\n",
              "         'seperate': 3,\n",
              "         'fits': 3,\n",
              "         'laughter': 4,\n",
              "         'school': 18,\n",
              "         'russian': 15,\n",
              "         'seeks': 8,\n",
              "         'democratic': 3,\n",
              "         'alaska’s': 1,\n",
              "         'house': 3,\n",
              "         'seat': 2,\n",
              "         'thousands': 10,\n",
              "         'march': 1,\n",
              "         'show': 21,\n",
              "         'support': 46,\n",
              "         'pronounce': 4,\n",
              "         'eyed': 1,\n",
              "         '↺rt❤': 4,\n",
              "         'last': 34,\n",
              "         'week': 34,\n",
              "         'justice': 26,\n",
              "         'staff': 10,\n",
              "         'members': 10,\n",
              "         'spoke': 6,\n",
              "         'focusing': 1,\n",
              "         'inhumane': 8,\n",
              "         'treatment': 11,\n",
              "         'poor': 12,\n",
              "         'conditions': 15,\n",
              "         'observed': 1,\n",
              "         'volunteering': 3,\n",
              "         'south': 6,\n",
              "         'family': 37,\n",
              "         'residential': 2,\n",
              "         'center': 28,\n",
              "         'dilley': 2,\n",
              "         'chaotic': 1,\n",
              "         'policy': 44,\n",
              "         'changes': 5,\n",
              "         'continue': 18,\n",
              "         'adversely': 1,\n",
              "         'affect': 3,\n",
              "         'detainees': 3,\n",
              "         ':': 8,\n",
              "         'vogue': 1,\n",
              "         'hijabi': 1,\n",
              "         'cover': 4,\n",
              "         'star': 12,\n",
              "         'growing': 11,\n",
              "         'migration': 49,\n",
              "         'y': 6,\n",
              "         'rub': 1,\n",
              "         'thought': 8,\n",
              "         \"you'd\": 3,\n",
              "         'try': 1,\n",
              "         '&;': 21,\n",
              "         'm': 4,\n",
              "         'hurts': 1,\n",
              "         'hardly': 3,\n",
              "         'sit': 5,\n",
              "         '\\u2066PERSON\\u2069': 15,\n",
              "         'times': 1,\n",
              "         'count': 5,\n",
              "         'don’t': 3,\n",
              "         'give': 1,\n",
              "         'opportunity': 10,\n",
              "         'choose': 5,\n",
              "         'healthcare': 5,\n",
              "         'civil': 13,\n",
              "         'social': 11,\n",
              "         'beasts': 1,\n",
              "         'quite': 9,\n",
              "         'exciting': 2,\n",
              "         '.”': 14,\n",
              "         'health': 11,\n",
              "         'uk': 16,\n",
              "         'original': 3,\n",
              "         'appreciate': 5,\n",
              "         'steal': 3,\n",
              "         'brim': 2,\n",
              "         'position': 4,\n",
              "         'fours': 1,\n",
              "         'pizza': 6,\n",
              "         'minute': 7,\n",
              "         'late': 2,\n",
              "         'libya': 32,\n",
              "         'tells': 8,\n",
              "         'italy:': 2,\n",
              "         '“they': 2,\n",
              "         'include': 2,\n",
              "         'human': 10,\n",
              "         'traffickers\"': 1,\n",
              "         'ever': 19,\n",
              "         'talkin': 2,\n",
              "         'someone': 51,\n",
              "         'say': 44,\n",
              "         'thing': 3,\n",
              "         'sudden': 4,\n",
              "         'desire': 1,\n",
              "         'ukraine': 5,\n",
              "         'grows': 1,\n",
              "         '22%': 1,\n",
              "         'her:i': 1,\n",
              "         'relationship': 4,\n",
              "         'him:well': 1,\n",
              "         'looking': 8,\n",
              "         'kept': 7,\n",
              "         'undercount': 2,\n",
              "         'direct': 3,\n",
              "         'resources': 3,\n",
              "         'counting': 4,\n",
              "         'researchers': 2,\n",
              "         'nikki': 2,\n",
              "         'haley:': 1,\n",
              "         \"'dictate'\": 1,\n",
              "         'program': 19,\n",
              "         'changing!': 1,\n",
              "         '$500k': 1,\n",
              "         'payment': 3,\n",
              "         'plans': 18,\n",
              "         'available': 5,\n",
              "         'citizen!': 1,\n",
              "         'visit': 11,\n",
              "         'contact': 3,\n",
              "         'gbibuildingcoPERSON': 1,\n",
              "         '.com': 1,\n",
              "         'afghan': 1,\n",
              "         'pakistani': 1,\n",
              "         'shot': 5,\n",
              "         'tried': 6,\n",
              "         'time': 1,\n",
              "         'txf': 1,\n",
              "         'least': 16,\n",
              "         'force': 10,\n",
              "         'against': 31,\n",
              "         'liquidates': 1,\n",
              "         'treasury': 1,\n",
              "         'holdings': 1,\n",
              "         'zero': 2,\n",
              "         'hedge': 4,\n",
              "         'surprise': 5,\n",
              "         'policies': 13,\n",
              "         'implications': 3,\n",
              "         'admit': 2,\n",
              "         'fuels': 2,\n",
              "         'funding': 1,\n",
              "         'pledges': 1,\n",
              "         'sigh': 3,\n",
              "         'game': 11,\n",
              "         '“the': 4,\n",
              "         'verification': 1,\n",
              "         'key': 7,\n",
              "         'establishing': 2,\n",
              "         'identities': 1,\n",
              "         'declared': 2,\n",
              "         'places': 8,\n",
              "         'origin': 4,\n",
              "         'myanmar': 3,\n",
              "         'voluntarily': 1,\n",
              "         'decide': 11,\n",
              "         'recently': 9,\n",
              "         'read': 36,\n",
              "         'book': 21,\n",
              "         'passage': 2,\n",
              "         'powerful': 5,\n",
              "         'ask': 5,\n",
              "         'author': 5,\n",
              "         'nah': 6,\n",
              "         'blurry': 1,\n",
              "         'move': 4,\n",
              "         'higher': 6,\n",
              "         'avert': 1,\n",
              "         'lives': 45,\n",
              "         'upcoming': 6,\n",
              "         'monsoon': 3,\n",
              "         'tesla': 1,\n",
              "         'driving': 1,\n",
              "         'running': 13,\n",
              "         'democrat': 1,\n",
              "         'alaska': 5,\n",
              "         'holla': 2,\n",
              "         'dissed': 1,\n",
              "         'misogynistic': 2,\n",
              "         'egotistical': 1,\n",
              "         'monger': 2,\n",
              "         'hiv': 3,\n",
              "         'positive': 8,\n",
              "         'mesa': 1,\n",
              "         'abused': 6,\n",
              "         '8': 15,\n",
              "         'authorities': 4,\n",
              "         'define': 2,\n",
              "         'witness': 2,\n",
              "         'hurt': 6,\n",
              "         '.\"': 15,\n",
              "         'welcoming': 8,\n",
              "         'stranger': 6,\n",
              "         'updated': 2,\n",
              "         'written': 7,\n",
              "         'matthew': 2,\n",
              "         'soerens': 1,\n",
              "         'jenny': 1,\n",
              "         'yang': 1,\n",
              "         'find': 31,\n",
              "         'copy': 3,\n",
              "         'today!': 8,\n",
              "         'grant': 6,\n",
              "         'first': 15,\n",
              "         'insult': 2,\n",
              "         '.about': 1,\n",
              "         'second': 15,\n",
              "         'though': 8,\n",
              "         'calling': 24,\n",
              "         \"doesn't\": 5,\n",
              "         'set': 13,\n",
              "         'there': 12,\n",
              "         '(and': 5,\n",
              "         'douchebags': 1,\n",
              "         'apparently': 12,\n",
              "         'meeting': 10,\n",
              "         '&amp;russia': 1,\n",
              "         'fm': 2,\n",
              "         'lavrov&amp;headrussian': 1,\n",
              "         'army': 6,\n",
              "         'in&amp;help': 1,\n",
              "         'rebuild': 17,\n",
              "         'wants': 10,\n",
              "         'sothat': 1,\n",
              "         'most': 16,\n",
              "         'materialize': 1,\n",
              "         'returnof': 1,\n",
              "         'german': 16,\n",
              "         'journo': 1,\n",
              "         'live': 26,\n",
              "         '29': 3,\n",
              "         'earned': 4,\n",
              "         '$13': 1,\n",
              "         '40': 2,\n",
              "         'hours': 8,\n",
              "         'bachelor': 1,\n",
              "         'mi': 4,\n",
              "         \"cc'rep\": 2,\n",
              "         'john': 7,\n",
              "         'moolenaar': 1,\n",
              "         'jail': 2,\n",
              "         'pray': 2,\n",
              "         'next': 15,\n",
              "         'wayyyyy': 1,\n",
              "         'relocationprogramm': 1,\n",
              "         'countries': 19,\n",
              "         'comfortabel': 1,\n",
              "         'i’ve': 1,\n",
              "         'wait': 5,\n",
              "         'food': 27,\n",
              "         'tolly': 1,\n",
              "         'ho': 21,\n",
              "         'friend': 29,\n",
              "         'carefree': 1,\n",
              "         'living': 14,\n",
              "         'drinking': 9,\n",
              "         'unprotected': 1,\n",
              "         'strangers': 5,\n",
              "         'problem': 10,\n",
              "         'stopped': 1,\n",
              "         'different': 6,\n",
              "         'virginia': 2,\n",
              "         'governor': 10,\n",
              "         'calls': 18,\n",
              "         'probe': 4,\n",
              "         'abuse': 25,\n",
              "         'allegations': 6,\n",
              "         'facility': 3,\n",
              "         'holds': 2,\n",
              "         'teens': 5,\n",
              "         'close': 1,\n",
              "         'friends': 24,\n",
              "         'other': 8,\n",
              "         'extra': 3,\n",
              "         'special': 4,\n",
              "         'daydream': 1,\n",
              "         'ya': 1,\n",
              "         'know': 27,\n",
              "         'xp': 1,\n",
              "         '10': 13,\n",
              "         'brother': 4,\n",
              "         'bc': 1,\n",
              "         'asked': 14,\n",
              "         'question': 1,\n",
              "         'va': 3,\n",
              "         'welcomes': 2,\n",
              "         'benefit': 3,\n",
              "         \"we'll\": 4,\n",
              "         'finland\"': 1,\n",
              "         'threaten': 3,\n",
              "         'finland': 2,\n",
              "         'better\"': 1,\n",
              "         'shoutout': 3,\n",
              "         'everyone': 1,\n",
              "         'accidentally': 2,\n",
              "         'fingers': 5,\n",
              "         'off': 28,\n",
              "         'video': 17,\n",
              "         'laugh': 8,\n",
              "         'patriotic': 2,\n",
              "         'politics': 6,\n",
              "         'asylum': 14,\n",
              "         'seekers': 14,\n",
              "         'designer': 1,\n",
              "         'housing': 9,\n",
              "         'nhs': 4,\n",
              "         'accessed': 1,\n",
              "         'hunger': 2,\n",
              "         'strike': 7,\n",
              "         'buy': 4,\n",
              "         'budgie': 1,\n",
              "         'publicity': 1,\n",
              "         'whi': 1,\n",
              "         'towards': 1,\n",
              "         'burma': 1,\n",
              "         're': 1,\n",
              "         'emerge': 1,\n",
              "         'top': 5,\n",
              "         'debate': 9,\n",
              "         'undocumented': 11,\n",
              "         'weekly': 2,\n",
              "         'address': 6,\n",
              "         'reformâ': 1,\n",
              "         'needed‘¦': 1,\n",
              "         'explore': 2,\n",
              "         'links': 3,\n",
              "         'poverty': 7,\n",
              "         'united': 8,\n",
              "         'founded': 2,\n",
              "         'dependent': 2,\n",
              "         'success': 4,\n",
              "         'well': 14,\n",
              "         'guess': 9,\n",
              "         'won': 3,\n",
              "         'woman\"': 1,\n",
              "         'award': 6,\n",
              "         'left': 13,\n",
              "         'spanish': 23,\n",
              "         'deny': 1,\n",
              "         \"'mass'\": 1,\n",
              "         'migration:': 1,\n",
              "         \"'europe\": 1,\n",
              "         \"blood'\": 1,\n",
              "         '.co/bnrq7n7oj3': 1,\n",
              "         'working': 1,\n",
              "         '@': 1,\n",
              "         'york': 11,\n",
              "         'three': 10,\n",
              "         'separation': 21,\n",
              "         'hugs': 1,\n",
              "         'germany': 2,\n",
              "         \"'lawless\": 1,\n",
              "         \"area':\": 1,\n",
              "         'police': 28,\n",
              "         'conducts': 1,\n",
              "         'marianna': 1,\n",
              "         '*': 10,\n",
              "         'status': 11,\n",
              "         'needed': 10,\n",
              "         '.however': 2,\n",
              "         'ha': 15,\n",
              "         'bjp': 2,\n",
              "         'knows': 1,\n",
              "         'hindus': 1,\n",
              "         'ready': 9,\n",
              "         'mandir': 8,\n",
              "         'taj': 1,\n",
              "         'mahal': 1,\n",
              "         'melissa': 2,\n",
              "         'giving': 4,\n",
              "         'happy': 7,\n",
              "         'dance': 1,\n",
              "         'parents': 35,\n",
              "         'stunning': 4,\n",
              "         'memoir': 1,\n",
              "         'divded': 1,\n",
              "         'blog': 16,\n",
              "         'post': 16,\n",
              "         'we’ve': 2,\n",
              "         'welcomed': 7,\n",
              "         'provided': 3,\n",
              "         'almost': 6,\n",
              "         '300': 3,\n",
              "         '1979': 2,\n",
              "         'love': 14,\n",
              "         'jesus': 6,\n",
              "         'also': 31,\n",
              "         'we’d': 2,\n",
              "         'it’s': 39,\n",
              "         'hurting': 3,\n",
              "         'repeatedly': 1,\n",
              "         'acts': 3,\n",
              "         'wasn’t': 4,\n",
              "         'being': 33,\n",
              "         'himself': 6,\n",
              "         'sip': 1,\n",
              "         'spill': 3,\n",
              "         'face': 5,\n",
              "         'bruh': 1,\n",
              "         'words': 3,\n",
              "         'describe': 6,\n",
              "         'artists': 7,\n",
              "         '.gosh': 1,\n",
              "         'vacation': 4,\n",
              "         'weeks': 11,\n",
              "         'paid': 6,\n",
              "         'vacations': 3,\n",
              "         'airfare': 3,\n",
              "         'claim': 6,\n",
              "         'fleeing': 10,\n",
              "         'unsafe': 1,\n",
              "         '“many': 1,\n",
              "         'removed': 4,\n",
              "         'result': 2,\n",
              "         'government’s': 2,\n",
              "         'inability': 2,\n",
              "         'failure': 1,\n",
              "         'track': 2,\n",
              "         '”': 7,\n",
              "         '[judge]': 1,\n",
              "         'sabraw': 2,\n",
              "         'said': 17,\n",
              "         'heard': 10,\n",
              "         '‘wavy': 1,\n",
              "         'bassline': 1,\n",
              "         'mix': 3,\n",
              "         '(halloween': 1,\n",
              "         'special’': 1,\n",
              "         'bank': 9,\n",
              "         'healthy': 2,\n",
              "         'conversation': 5,\n",
              "         'ht': 2,\n",
              "         'cube': 6,\n",
              "         'shitty': 4,\n",
              "         'company': 8,\n",
              "         'edawn': 2,\n",
              "         'hyuna': 3,\n",
              "         'went': 4,\n",
              "         'rules': 2,\n",
              "         '.ofc': 1,\n",
              "         'sm': 1,\n",
              "         ...})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "non_hate_words__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "rodrUyuRJqQq",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Are there any words that are present in one but not in the other?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg460T13OE9R"
      },
      "source": [
        "<a name='e5'></a>\n",
        "### Exercise 5 (points 5)\n",
        "\n",
        "Fill in the following function to return (a Counter of) words that are not in the provided vocabulary. The vocabulary is in the form of a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8WKJ3eApJqQq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_oov_word_count(word_count, vocab):\n",
        "    \"\"\"\n",
        "    Finds the words that are not in the provided vocabulary.\n",
        "    Args:\n",
        "        word_count: Counter of the words\n",
        "        vocab: a list of words in the vocabulary\n",
        "\n",
        "    Returns: a Counter of the words that are outside the vocabulary\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    diff = word_count - vocab\n",
        "\n",
        "    # return your Counter here\n",
        "    return diff\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0MTLichSJqQq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# non_hate_only_word_count = get_oov_word_count(non_hate_word_count, hate_word_count.keys())\n",
        "# hate_only_word_count = get_oov_word_count(hate_word_count, non_hate_word_count.keys())\n",
        "\n",
        "# print(f'only non-hate word count: {len(non_hate_only_word_count)}')\n",
        "# print(f'only hate word count: {len(hate_only_word_count)}')\n",
        "\n",
        "# print('Most common non-hate words:')\n",
        "# print(non_hate_only_word_count.most_common(10))\n",
        "\n",
        "# print('Most common hate words:')\n",
        "# print(hate_only_word_count.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "only non-hate word count: 11171\n",
            "only hate word count: 6339\n",
            "Most common non-hate words:\n",
            "[('.', 1340), ('the', 932), (',', 901), ('of', 779), ('to', 640), ('immigrant', 537), ('in', 494), ('and', 460), ('you', 392), ('for', 368)]\n",
            "Most common hate words:\n",
            "[('HASHTAG', 1319), ('bitch', 600), ('illegal', 249), ('!', 164), ('hoe', 148), ('whore', 127), ('ass', 122), ('illegals', 113), ('do', 91), ('they', 86)]\n"
          ]
        }
      ],
      "source": [
        "non_hate_only_word_count = get_oov_word_count(non_hate_word_count, hate_word_count) # i removed the keys thing\n",
        "hate_only_word_count = get_oov_word_count(hate_word_count, non_hate_word_count) # it complicates it and is not necessary\n",
        "\n",
        "print(f'only non-hate word count: {len(non_hate_only_word_count)}')\n",
        "print(f'only hate word count: {len(hate_only_word_count)}')\n",
        "\n",
        "print('Most common non-hate words:')\n",
        "print(non_hate_only_word_count.most_common(10))\n",
        "\n",
        "print('Most common hate words:')\n",
        "print(hate_only_word_count.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "v8NyFBOAJqQq",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Often some words are used rarely in a specific context. We can remove them based on a threshold (a minimum number of occurrences that are required to keep the word in).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgffQ6JyOL-C"
      },
      "source": [
        "<a name='e6'></a>\n",
        "### Exercise 6 (points 4)\n",
        "\n",
        "Fill in the following function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "a6JKSPwUJqQq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def prune_word_count(word_count, threshold):\n",
        "    \"\"\"\n",
        "    Removes words from the word counter that occur less than the specified threshold\n",
        "    Args:\n",
        "        word_count: Counter with the word counts\n",
        "        threshold: a minimum number of occurrences\n",
        "\n",
        "    Returns: a Counter with frequent words\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    copy_dict = word_count.copy() # need copy otherwise python is not happy\n",
        "    \n",
        "    for k, v in word_count.items(): # iterate over dict\n",
        "        if v < threshold: # check if count under threshold\n",
        "            copy_dict.pop(k) # remove item\n",
        "\n",
        "    # return your Counter here\n",
        "    return copy_dict\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MQAjjWtMJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "pruned_word_count = prune_word_count(train_word_count, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "u011BldCJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train word count: 18867\n",
            "pruned word count: 4174\n",
            "Most common words:\n",
            "[('.', 7690), ('HASHTAG', 6127), ('PERSON', 6022), ('the', 5464), ('to', 4778), (',', 4489), ('you', 4068), ('a', 3912)]\n",
            "Least common words:\n",
            "[('bankrupt', 4), ('tourists', 4), ('insulting', 4), ('memorial', 4), ('farmers', 4), ('harper', 4), ('nerve', 4), ('complex', 4)]\n"
          ]
        }
      ],
      "source": [
        "print(f'train word count: {len(train_word_count)}')\n",
        "print(f'pruned word count: {len(pruned_word_count)}')\n",
        "\n",
        "print('Most common words:')\n",
        "print(pruned_word_count.most_common(8))\n",
        "\n",
        "print('Least common words:')\n",
        "print(pruned_word_count.most_common()[-8:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "U3UfnuzdJqQr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now (finally) we will construct a vocabulary. It will be a list of words (sorted alphabetically) that are present in the pruned word counter of the training subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sOoJT1qPJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 4174\n",
            "['!', '\"', '#', '$', '$$', '$$$', '$1', '$100', '$135', '$2']\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(pruned_word_count.keys())\n",
        "vocab_size = len(vocab)\n",
        "print(f'Vocab size: {vocab_size}')\n",
        "print(vocab[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4UvEoGZMfMO"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4. Build a Handcrafted Classifier\n",
        "\n",
        "Recall from the lectures that a classification model, is practically a function that maps the input text to the output classes. In our case, the output classes are 2: non-hate (return ```0```) or hate (return ```1```). In this section, we will build such a function, based on a rule-based system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a07uoNmv8jS"
      },
      "source": [
        "<a name='e7'></a>\n",
        "### Exercise 7 (points 5)\n",
        "\n",
        "In order to device such simple classification rules, we will use analysis we conducted previously about which words are frequent in which class (i.e. non-hate vs. hate). Try to figure out an algorithm (based on if-else statements, aka rules) that would detect hate speech. Code the algorithm in the following function. You should not try to be exhaustive here and come up with the perfect rules (we are not sure they exist) but it's important to reflect (at the next exercise) on the trade-offs of implementing a rule-based classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**suggestion for the next labs -> please tell us what type of arguments the function takes**\n",
        "**i cannot figure out from looking at the function what \"text\" is**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERSON nice new signage . are you not concerned by beatlemania style hysterical crowds crongregating on you .\n",
            "a woman who you fucked multiple times saying you dick small is a compliment you know you hit that spot\n",
            "PERSON PERSON real talk do you have eyes or were they gouged out by a rapefugee ?\n",
            "your girlfriend lookin at me like a groupie in this bitch!\n",
            "hysterical woman like PERSON\n",
            "me flirting so tell me about your father .\n",
            "the philippine catholic bishops' work for migrant workers should focus on families who are paying the great .\n",
            "i am not going after your ex bf you lieing sack of shit ! i'm done with you dude that why i dumped your ass cause your a lieing  bitch\n",
            "when cuffing season is finally over\n",
            "send home migrants not in need of protection , peter dutton tells un , hey dutton how about the ones that have stayed and not left the country when they should overstayers ? why do not you go and round all them up ?\n"
          ]
        }
      ],
      "source": [
        "for i in tweet_ds['train']['clean'][0:10]:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('HASHTAG', 1319),\n",
              " ('bitch', 600),\n",
              " ('illegal', 249),\n",
              " ('!', 164),\n",
              " ('hoe', 148),\n",
              " ('whore', 127),\n",
              " ('ass', 122),\n",
              " ('illegals', 113),\n",
              " ('do', 91),\n",
              " ('they', 86),\n",
              " ('fuck', 84),\n",
              " ('want', 76),\n",
              " ('stop', 76),\n",
              " ('no', 71),\n",
              " ('country', 70),\n",
              " ('her', 69),\n",
              " ('fucking', 69),\n",
              " ('bitches', 68),\n",
              " ('she', 61),\n",
              " ('alien', 61),\n",
              " ('aliens', 61),\n",
              " ('stupid', 57),\n",
              " ('deport', 57),\n",
              " ('shut', 55),\n",
              " ('wall', 54),\n",
              " ('HASHTAGHASHTAG', 54),\n",
              " ('these', 53),\n",
              " ('cunt', 51),\n",
              " ('europe', 51),\n",
              " ('hoes', 49),\n",
              " ('pussy', 47),\n",
              " ('suck', 45),\n",
              " ('them', 43),\n",
              " ('come', 43),\n",
              " ('.HASHTAG', 42),\n",
              " ('why', 40),\n",
              " ('citizens', 37),\n",
              " ('get', 36),\n",
              " ('girls', 36),\n",
              " ('illegally', 35),\n",
              " ('usa', 35),\n",
              " ('must', 34),\n",
              " ('borders', 34),\n",
              " ('let', 33),\n",
              " ('build', 33),\n",
              " ('americans', 33),\n",
              " ('dumb', 30),\n",
              " ('ur', 29),\n",
              " ('now', 28),\n",
              " ('immigrants', 28)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hate_words__.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "hate_words_ = [x[0] for x in hate_only_word_count.most_common(50)] # get into list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['HASHTAG',\n",
              " 'bitch',\n",
              " 'illegal',\n",
              " '!',\n",
              " 'hoe',\n",
              " 'whore',\n",
              " 'ass',\n",
              " 'illegals',\n",
              " 'do',\n",
              " 'they',\n",
              " 'fuck',\n",
              " 'want',\n",
              " 'stop',\n",
              " 'no',\n",
              " 'country',\n",
              " 'her',\n",
              " 'fucking',\n",
              " 'bitches',\n",
              " 'she',\n",
              " 'alien',\n",
              " 'aliens',\n",
              " 'stupid',\n",
              " 'deport',\n",
              " 'shut',\n",
              " 'wall',\n",
              " 'HASHTAGHASHTAG',\n",
              " 'these',\n",
              " 'cunt',\n",
              " 'europe',\n",
              " 'hoes',\n",
              " 'pussy',\n",
              " 'suck',\n",
              " 'them',\n",
              " 'come',\n",
              " '.HASHTAG',\n",
              " 'why',\n",
              " 'citizens',\n",
              " 'get',\n",
              " 'girls',\n",
              " 'illegally',\n",
              " 'usa',\n",
              " 'must',\n",
              " 'borders',\n",
              " 'let',\n",
              " 'build',\n",
              " 'americans',\n",
              " 'dumb',\n",
              " 'ur',\n",
              " 'now',\n",
              " 'immigrants']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hate_words_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5IGmwIbSNLJc"
      },
      "outputs": [],
      "source": [
        "def detect_hate(text):\n",
        "  bow = get_bag_of_words(text) # this is a counter (dict)\n",
        "  \n",
        "  ### YOUR CODE HERE\n",
        "  label = 0\n",
        "\n",
        "  # pruned_bow = pruned_word_count(bow, 4) # i remove all the uncommon words\n",
        "\n",
        "  hate_words_ = [x[0] for x in hate_only_word_count.most_common(50)] # get into list\n",
        "  hate_words_.remove('usa')\n",
        "  hate_words_.remove('citizens')\n",
        "  hate_words_.remove('europe')\n",
        "  hate_words_.remove('girls')\n",
        "  hate_words_.remove('she')\n",
        "  hate_words_.remove('her')\n",
        "  hate_words_.remove('no')\n",
        "  hate_words_.remove('do')\n",
        "  hate_words_.remove('they')\n",
        "  hate_words_.remove('these')\n",
        "  hate_words_.remove('!')\n",
        "  \n",
        "  keys = list(bow.keys())\n",
        "\n",
        "  for word in keys:\n",
        "    if word in hate_words_: # the word is in the hate word category\n",
        "      label = 1\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  # Don't forget to return:\n",
        "  # * 0 for non-hate\n",
        "  # * 1 for hate\n",
        "  return label\n",
        "\n",
        "  ### YOUR CODE ENDS HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6oSQTXqOEHi"
      },
      "source": [
        "The next code block will apply your function to the dataset to obtain the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2zl10-5IOLa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 8993/8993 [00:03<00:00, 2527.03 examples/s]\n",
            "Map: 100%|██████████| 2970/2970 [00:01<00:00, 2495.06 examples/s]\n",
            "Map: 100%|██████████| 999/999 [00:00<00:00, 2496.68 examples/s]\n"
          ]
        }
      ],
      "source": [
        "def apply_hate_speech_detector(example):\n",
        "  text = example['clean']\n",
        "  predicted_label = detect_hate(text)\n",
        "  example['predicted_label'] = predicted_label\n",
        "  return example\n",
        "\n",
        "# apply the function to all examples\n",
        "predictions = tweet_ds.map(apply_hate_speech_detector)\n",
        "# remove columns other than 'predicted_label'\n",
        "predictions = predictions.remove_columns(tweet_ds['train'].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIgC2sP7Onx-"
      },
      "source": [
        "Here we create a small function to print the F1 score for predictions of the model. We will use it shortly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iixOOsbJJqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "\n",
        "def print_scores(y, prediction, report=False):\n",
        "    print('f1:', f1_score(y, prediction, average='macro'))\n",
        "    if report:\n",
        "        print(classification_report(y, prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGbPx_7mnjQp"
      },
      "source": [
        "Let's retrieve the predictions for each subset (train, validation, test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Pq83XswoPTvd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['predicted_label'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['predicted_label'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['predicted_label'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n",
            "train\n",
            "(8993,)\n",
            "(8993,)\n",
            "valid\n",
            "(999,)\n",
            "(999,)\n",
            "test\n",
            "(2970,)\n",
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "print(predictions)\n",
        "\n",
        "train_prediction = predictions['train']['predicted_label']\n",
        "y_train = tweet_ds['train']['label']\n",
        "print('train')\n",
        "print(train_prediction.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "valid_prediction = predictions['validation']['predicted_label']\n",
        "y_valid = tweet_ds['validation']['label']\n",
        "print('valid')\n",
        "print(valid_prediction.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "test_prediction = predictions['test']['predicted_label']\n",
        "y_test = tweet_ds['test']['label']\n",
        "print('test')\n",
        "print(test_prediction.shape)\n",
        "print(y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwJzn9CInrY-"
      },
      "source": [
        "Here we will print the F1 scores of your predictions for each subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PucCvtHGU477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.6378455001031347\n",
            "validation score:\n",
            "f1: 0.5927075017843175\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.46      0.56       572\n",
            "           1       0.52      0.78      0.62       427\n",
            "\n",
            "    accuracy                           0.59       999\n",
            "   macro avg       0.62      0.62      0.59       999\n",
            "weighted avg       0.64      0.59      0.59       999\n",
            "\n",
            "test score:\n",
            "f1: 0.330426404259123\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.04      0.07      1718\n",
            "           1       0.42      0.97      0.59      1252\n",
            "\n",
            "    accuracy                           0.43      2970\n",
            "   macro avg       0.53      0.50      0.33      2970\n",
            "weighted avg       0.54      0.43      0.29      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('training score:')\n",
        "print_scores(y_train, train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, valid_prediction, report=True)\n",
        "print('test score:')\n",
        "print_scores(y_test, test_prediction, report=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HdMM9LiZ14G"
      },
      "source": [
        "Let us also look at some examples with their predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "52a8ewRoZ1fY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@user , you are correct that Reid certainly is a weasel. Sadly, we've got our own weasels; @user Sen McConnell & @user .The corrupt Mueller investigation w/be STOPPED if those 3 did their jobs.#MAGA #KAG #POTUS #Trump #NEWS #VoteRed #NoDACA #USA\n",
            "Predicted label: 1\n",
            "True label: 0\n",
            "\n",
            "Whoever just unfollowed me you a bitch\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "@user @user Those People Invaded Us!!! They DO NOT BELING HERE & HAVE NO RIGHTS! Its #AmericaFIRST! Open Your House To Them If Your That IGNORANT! & Yes Im A #Christian Too! #NODACA!\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "stop JUDGING bitches by there cover, jus cuz she bad don't mean she's a catch shawdy could be a whore 👀 das opposite of a keeper\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "how about i knock heads off and send them gift wrapped to your moms house you dumb raggedy bird bitch ass hoes\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "@user @user @user Always #NoDACA.I AM BORN IN #USA AND #USA FIRST.\n",
            "Predicted label: 1\n",
            "True label: 0\n",
            "\n",
            "@user friends ? that's my BLOOD brother bitch .. he in yo city !! i'm hours away . ill be through there tomo…\n",
            "Predicted label: 1\n",
            "True label: 0\n",
            "\n",
            "@user @user Muslims attacked US on 9/11, 3000 killed. Subsequently we allowed more Muslims in our country?!?Does this make any sense to you?You can thank Obama, his liberals minions, RINO's & political correctness. #MuslimBan #BanIslam #TravelBan #DrainTheDeepState #DeportThemAll\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "@user @user Like he ever kept out any threats. He's lying as usual. #BuildThatWall\n",
            "Predicted label: 1\n",
            "True label: 0\n",
            "\n",
            "#germany deserves to called #cuck/land they allow in #isis fighters as #Refugees even if they raped underaged girls. this #yazidi girl is scared of this and she is forced to see the face of her rapist.#refugeesnotwelcome should apply to these scum\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    example = tweet_ds['test'][i]\n",
        "    print(example['text'])\n",
        "    print(f'Predicted label: {test_prediction[i]}')\n",
        "    print(f'True label: {y_test[i]}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bdJ8hCqXJpx"
      },
      "source": [
        "<a name='e8'></a>\n",
        "### Exercise 8 (points 5)\n",
        "\n",
        "In this section, discuss the results you obtained. Potentially, try to improve your algorithm based on the results (e.g. do you have many false positives? or false negatives?). Make sure to comment on the trade-offs of such a simplified system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_matrix = confusion_matrix(y_test, test_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  65 1653]\n",
            " [  38 1214]]\n"
          ]
        }
      ],
      "source": [
        "print(test_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki0sOBPbaQjJ"
      },
      "source": [
        "**ANSWER**\n",
        "\n",
        "the f1 score on the test set is only about 0.34. This suggests there is a lot of improvements that can be made.\n",
        "\n",
        "What i thought about doing first is maybe make word pairs, to see if any hate word is preceded by a non hate word, etc. This wouldn't prove to be highly efficient in practice however.\n",
        "\n",
        "I also thought about setting a threshold for hate speech, so, if more than 1 word is hate, then mark the sentence as hate speech. This won't work if the sentence is short. \n",
        "\n",
        "Ultimately what this classifier is missing is context, if we don't have any understanding of the context in which words are placed, we cannot predict hate speech correctly with high accuracy. \n",
        "\n",
        "I did improve it slightly, by removing words from the hate list, such as \"girls\", \"usa\" ..\n",
        "\n",
        "**on the valid set**\n",
        "\n",
        "True negatives (TN): 1267\n",
        "\n",
        "False positives (FP): 451\n",
        "\n",
        "False negatives (FN): 968\n",
        "\n",
        "True positives (TP): 284\n",
        "\n",
        "It seems this classifier fails to classify hate speech correctly, looking at the false negatives rate. This can be explained by the fact that many of the words in the \"hate_words\" list are probably not inherently hate speech at all. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "O685cntcJqQr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5. Build Bag-of-Words\n",
        "\n",
        "Having a vocabulary is important for indexing every word in our corpus.\n",
        "To represent a document as a vector however, we need more than just indexing, such as a vector space that represents the words:\n",
        "\n",
        "* Bag-of-Words model: A single document can be considered as a bag of words and how many times each word occured, without caring about the order of the words. The word occurence counting is also called term frequency. You can think if this as a vector over all of the vocabulary where the entries are how many times that term has occured.\n",
        "\n",
        "* TF-IDF: term frequency–inverse document frequency diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Mu0gxIhaJqQr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e9'></a>\n",
        "### Exercise 9 (points 5)\n",
        "\n",
        "Let us build the bag-of-words for the whole dataset. We will only include the words that are present in our vocabulary that we created in the previous section. Words that are missing from the vocabulary will be replaced with the <unknown> token. For training we need to convert the counter into a numpy array [https://numpy.org/doc/stable/reference/generated/numpy.array.html](https://numpy.org/doc/stable/reference/generated/numpy.array.html). The array will be of the size of the vocabulary (plus 1 for the <unknown> token). Each element will correspond to the word in our vocabulary and contain the number of occurrences of this word in a particular sentence. Fill in the following function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wsRwR-pKJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def calculate_bag_of_words(example):\n",
        "    \"\"\"\n",
        "    Calculates the word count and encodes it as a numpy array of the size equal to the vocab\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: updated example with 'bow' column\n",
        "\n",
        "    \"\"\"\n",
        "    text = example['clean']\n",
        "    bow = get_bag_of_words(text)\n",
        "    # create numpy array with the size of the vocab plus 1 for the unknown token\n",
        "    bow_numpy = np.zeros((vocab_size + 1), dtype=int)\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    for k, v in bow.items(): # this iterates over dict\n",
        "        idx = text.index(k)\n",
        "        bow_numpy[idx] = v\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n",
        "    example['bow'] = bow_numpy\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3_eN_JHcJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on you…', 'label': 0, 'clean': 'PERSON nice new signage . are you not concerned by beatlemania style hysterical crowds crongregating on you .', 'bow': array([1, 0, 0, ..., 0, 0, 0])}\n"
          ]
        }
      ],
      "source": [
        "print(calculate_bag_of_words(tweet_ds['train'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Ldq19LLiJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 8993/8993 [00:01<00:00, 8820.06 examples/s]\n",
            "Map: 100%|██████████| 2970/2970 [00:00<00:00, 8896.87 examples/s]\n",
            "Map: 100%|██████████| 999/999 [00:00<00:00, 9256.76 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 0 0 ... 0 0 0]\n",
            " [2 0 1 ... 0 0 0]\n",
            " [2 0 0 ... 0 0 0]\n",
            " ...\n",
            " [1 0 0 ... 0 0 0]\n",
            " [6 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.map(calculate_bag_of_words)\n",
        "\n",
        "print(tweet_ds['train']['bow'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PXRJV4B8JqQs",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='6'></a>\n",
        "## 6. Bag-of-Words with Naive Bayes\n",
        "\n",
        "Our first classification algorithm will be the Naive-Bayes algorithm. Naive Bayes is a generative classification algorithm: the model assigns class labels to the input text, which is represented as a vectors of feature values (this will be our bag-of-words).\n",
        "\n",
        "If you need a refresher on the method, please revisit the lecture slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5K7JlqjpvTV"
      },
      "source": [
        "We can start by extracting the features and labels from our dataset. We will use it to train the classifier (on 'train' subset) and evaluate it after training (on 'valid' and 'test' subsets).\n",
        "\n",
        "The dataset will return the features as numpy arrays. We can inspect their shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Gw1DedUPJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8993, 4175)\n",
            "(8993,)\n",
            "(999, 4175)\n",
            "(999,)\n",
            "(2970, 4175)\n",
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "X_train = tweet_ds['train']['bow']\n",
        "y_train = tweet_ds['train']['label']\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "X_valid = tweet_ds['validation']['bow']\n",
        "y_valid = tweet_ds['validation']['label']\n",
        "\n",
        "print(X_valid.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "X_test = tweet_ds['test']['bow']\n",
        "y_test = tweet_ds['test']['label']\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PbyEjcSp706"
      },
      "source": [
        "The scikit-learn library also contains the implementation of Naive Bayes. You will use it in the next exercise. Here is the documentation [https://scikit-learn.org/stable/modules/naive_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html). The model is trained by calling the ```fit()``` method. It receives the features of the training examples and their labels. Depending on your machine, the training might take from several seconds to up to a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "VaNamSHYJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBHHQrwhOy9l"
      },
      "source": [
        "We can use the trained classifier to make predictions. The method ```predict()``` will output the predictions made by the model. We can score them and print the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GBs-atNhJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.546000516176377\n",
            "validation score:\n",
            "f1: 0.529766348163631\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.64      0.62       572\n",
            "           1       0.47      0.42      0.44       427\n",
            "\n",
            "    accuracy                           0.55       999\n",
            "   macro avg       0.53      0.53      0.53       999\n",
            "weighted avg       0.54      0.55      0.54       999\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nb_train_prediction = nb_classifier.predict(X_train)\n",
        "nb_valid_prediction = nb_classifier.predict(X_valid)\n",
        "\n",
        "print('training score:')\n",
        "print_scores(y_train, nb_train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, nb_valid_prediction, report=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4xSxoIfpdAC"
      },
      "source": [
        "Let's do the same for the test subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "63snGggAJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.546000516176377\n",
            "validation score:\n",
            "f1: 0.529766348163631\n",
            "test score:\n",
            "f1: 0.504838513617073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.67      0.62      1718\n",
            "           1       0.43      0.35      0.39      1252\n",
            "\n",
            "    accuracy                           0.53      2970\n",
            "   macro avg       0.51      0.51      0.50      2970\n",
            "weighted avg       0.52      0.53      0.52      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nb_test_prediction = nb_classifier.predict(X_test)\n",
        "print('training score:')\n",
        "print_scores(y_train, nb_train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, nb_valid_prediction)\n",
        "print('test score:')\n",
        "print_scores(y_test, nb_test_prediction, report=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1143,  575],\n",
              "       [ 813,  439]])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_matrix = confusion_matrix(y_test, nb_test_prediction)\n",
        "test_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "p3Esqe7rJqQt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e10'></a>\n",
        "### Exercise 10 (points 10)\n",
        "\n",
        "Analyze and comment on the results. Compare to the previous results. Do you see anything unexpected? Provide explanations if so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy2whYbSqab6"
      },
      "source": [
        "**results**\n",
        "\n",
        "This bag of words approach gives me better results than the rule-based approach does. In fact, the score on the test set for the rule based approach was only slightly above 30%, way worse than random guessing would predict.\n",
        "\n",
        "In the case of the bag of words approach, the f1 score on the test set is not statistically different than random guessing (in which case f1 is 0.5).\n",
        "\n",
        "Both approach are not great, although I suspect some further tuning would potentially help both approaches reach higher test f1 scores.\n",
        "\n",
        "- True Negative (TN): 1143\n",
        "- False Positive (FP): 575\n",
        "- False Negative (FN): 813\n",
        "- True Positive (TP): 439\n",
        "\n",
        "The above statistics suggest the model is decent at detecting non-hate speech, but is very bad at correctly identifying hate speech. Overall, nothing is really unexpected here, we used a different classifier, we got better results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7gS-N5paJqQr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='7'></a>\n",
        "## 7. Bag-of-Words with Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Phlb4uNuFH"
      },
      "source": [
        "In this section, we will apply a different classification algorithm, namely Logistic Regression. If you need a refresher about Logistic Regression, make sure to revisit the lecture slides.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWtMVF8OODxb"
      },
      "source": [
        "You can check the documentation of the LogisticRegression here [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). The model is trained by calling the ```fit()``` method. It receives the features of the training examples and their labels. Depending on your machine, the training might take from several seconds to up to a minute or two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "in-_9XlTJqQs",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e11'></a>\n",
        "### Exercise 11 (points 3)\n",
        "\n",
        "Train Logistic Regression on the bag-of-words features.\n",
        "First, instantiate the classifier (visit the documentation to find the parameters of the constructor [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). Next, train the classifier on the training data (```X_train``` and ```y_train```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "-AeED0K0JqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#### YOUR CODE HERE\n",
        "\n",
        "lr_classifier = LogisticRegression(max_iter = 1000)\n",
        "\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvD7nlBdQefX"
      },
      "source": [
        "Now, let's see how the model is performing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "nxak5F5QJqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.5435495289544228\n",
            "validation score:\n",
            "f1: 0.513433257918552\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.78      0.67       572\n",
            "           1       0.49      0.28      0.35       427\n",
            "\n",
            "    accuracy                           0.57       999\n",
            "   macro avg       0.54      0.53      0.51       999\n",
            "weighted avg       0.55      0.57      0.54       999\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_prediction = lr_classifier.predict(X_train)\n",
        "valid_prediction = lr_classifier.predict(X_valid)\n",
        "\n",
        "print('training score:')\n",
        "print_scores(y_train, train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, valid_prediction, report=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAhZA3gEPRBk"
      },
      "source": [
        "Let's do the same for the test subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "F1JKt2YNJqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.5435495289544228\n",
            "validation score:\n",
            "f1: 0.513433257918552\n",
            "test score:\n",
            "f1: 0.46739893330453897\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.74      0.65      1718\n",
            "           1       0.39      0.23      0.29      1252\n",
            "\n",
            "    accuracy                           0.53      2970\n",
            "   macro avg       0.48      0.49      0.47      2970\n",
            "weighted avg       0.50      0.53      0.50      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_prediction = lr_classifier.predict(X_test)\n",
        "\n",
        "print('training score:')\n",
        "print_scores(y_train, train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, valid_prediction)\n",
        "print('test score:')\n",
        "print_scores(y_test, test_prediction, report=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "8v-bU9tbsxu3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "print(test_prediction.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxwawpz8Paia"
      },
      "source": [
        "You can also inspect the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-4WAKgEUJqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@user , you are correct that Reid certainly is a weasel. Sadly, we've got our own weasels; @user Sen McConnell & @user .The corrupt Mueller investigation w/be STOPPED if those 3 did their jobs.#MAGA #KAG #POTUS #Trump #NEWS #VoteRed #NoDACA #USA\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "Whoever just unfollowed me you a bitch\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n",
            "@user @user Those People Invaded Us!!! They DO NOT BELING HERE & HAVE NO RIGHTS! Its #AmericaFIRST! Open Your House To Them If Your That IGNORANT! & Yes Im A #Christian Too! #NODACA!\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n",
            "stop JUDGING bitches by there cover, jus cuz she bad don't mean she's a catch shawdy could be a whore 👀 das opposite of a keeper\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n",
            "how about i knock heads off and send them gift wrapped to your moms house you dumb raggedy bird bitch ass hoes\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n",
            "@user @user @user Always #NoDACA.I AM BORN IN #USA AND #USA FIRST.\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "@user friends ? that's my BLOOD brother bitch .. he in yo city !! i'm hours away . ill be through there tomo…\n",
            "Predicted label: 1\n",
            "True label: 0\n",
            "\n",
            "@user @user Muslims attacked US on 9/11, 3000 killed. Subsequently we allowed more Muslims in our country?!?Does this make any sense to you?You can thank Obama, his liberals minions, RINO's & political correctness. #MuslimBan #BanIslam #TravelBan #DrainTheDeepState #DeportThemAll\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "@user @user Like he ever kept out any threats. He's lying as usual. #BuildThatWall\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "#germany deserves to called #cuck/land they allow in #isis fighters as #Refugees even if they raped underaged girls. this #yazidi girl is scared of this and she is forced to see the face of her rapist.#refugeesnotwelcome should apply to these scum\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    example = tweet_ds['test'][i]\n",
        "    print(example['text'])\n",
        "    print(f'Predicted label: {test_prediction[i]}')\n",
        "    print(f'True label: {y_test[i]}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1279  439]\n",
            " [ 966  286]]\n"
          ]
        }
      ],
      "source": [
        "print(confusion_matrix(y_test, test_prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lBG9KQzjJqQs",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e12'></a>\n",
        "### Exercise 12 (points 10)\n",
        "\n",
        "Analyze and comment on the results. Do you see anything unexpected? Give explanations.\n",
        "\n",
        "Analyze at least 10 errors from the results (could be either false positives or negatives) and try to explain why they occured (e.g. was it because of some linguistic phenomenon that cannot be captured by the models, was it because of bad annotation etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmLbQfC9qcOV"
      },
      "source": [
        "**results**\n",
        "\n",
        "The logistic regression classifier attempts to correctly classify by learning patterns of hate speech in the data provided. In this case, it does not perform well at all, which is very much expected, the f1 score on the test set is 0.46, slightly worse than random guessing. \n",
        "\n",
        "\"#germany deserves to called #cuck/land they allow in #isis fighters as #Refugees even if they raped underaged girls (...)\"\n",
        "-> obvious hate speech, yet the classifier doesn't report this as hate speech. This is a false negative. Most likely why this occured is because some hate words might be entirely new to the test set itself, because they are quite obviously hate oriented for the most part.\n",
        "\n",
        "\"Whoever just unfollowed me you a bitch\"\n",
        "-> This is labeled as hate speech, the classifier classifies it as not hate speech (false negative). It is however more nuanced here whether this really is hate speech or not. I would tend to say it is not, maybe it is incorrectly labeled. \n",
        "\n",
        "\"@user @user @user Always #NoDACA.I AM BORN IN #USA AND #USA FIRST.\"\n",
        "-> this was predicted as non hate speech, and labeled as non hate speech (true negative). However, the context of this tweet seems to indicate hate speech. #NoDACA opposes to certain immigration policies, which in itself is not hateful, but used here, with the \"always #NoDACA, #USA FIRST\", i would tend to say this is hate speech, and it is wrongly labeled.\n",
        "\n",
        "\"@user @user Toronto is starting to sound like LondonstanWe have to get on the #MadMax2019 train!!@MaximeBernier is the only one who can get our country back.#BanMuslims #BanIslam #SendThemBack\"\n",
        "-> this is classified by the model as hate speech, but labeled as non-hate speech. \"ban muslims, ban islam, send them back\" is more than enough to label this as hate speech. Technically this is a fase positive, but it is labeled incorrectly to begin with. \n",
        "\n",
        "\"@user We all know what you mean by 'scum' pal. Couldn't agree more. #SendThemBack\"\n",
        "-> very similar to the case above, but this time the model classifies this as non-hate speech, while it is labeled as hate speech (false negative). Here, it is obvious that the model doesn't recognize \"#SendThemBack\" as a token of hate speech, and fails to understand the context of the sentence, because maybe if we ommit the hashtag, this isn't hate speech.\n",
        "\n",
        "\"@user BITCH I LOVE YOU WITH MY WHOLE HEART UR MY FAVE PERSON EVER THANK U ❤️\"\n",
        "-> model classifies this as hate speech, while it is labeled otherwise (false positive). It is obivous that this is an example of a lack of context understanding from the model. It sees the word \"bitch\", and classifies this as hate immediately, when it is obviously not hate speech. \n",
        "\n",
        "\"#newday they are not asylum seekers they are illegal immigrants #sendthemback\"\n",
        "-> model does not classify this as hate speech, when it is labeled as such (false negative). This again shows that the model does not take hashtags into account at all, because the sentence before the hashtag is not necessarily hate speech. \n",
        "\n",
        "\"When some1 is very sick, you send a doctor, not ask them to come to hospital themselves. Now explain me why are we bringing in Syrians, especially adults, into Turkey letting them steal our jobs, lower the wages and ridicule our values?#sendthemback #syrian #Refugees #Turkey\"\n",
        "-> the model classifies this as non hate speech, when it is labeled as hate speech (false negative). What is happening here is again a misunderstanding of context from the model. None of the words in the sentence are not indicative of hate on their own, which is why the classifier fails to see this as hate speech.\n",
        "\n",
        "\"@user Please visit today to honor the thousands of victims killed by illegal aliens!We are building a memorial wall.#RETWEETHelp #BuildThatWall in their memory!Follow @user & @user\"\n",
        "-> the model classifies this as hate speech, while it is not labeled as such (false positive). This however seems to be an issue with the label itself, as the sentence is quite obviously hate speech.\n",
        "\n",
        "\"Shut your dumbass up bitch we all know you a hoe\"\n",
        "-> model classifies this as non hate speech, when it is labeled as such. I am not sure why this is, maybe this is a case of unseen data. (false negative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KHtLxw-dJqQt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='8'></a>\n",
        "## 8. TF-IDF\n",
        "Comment on the above results and try to propose/come up with some improvements. Explore the possibilities of the bag-of-words and different variations (e.g. TF-IDF). Make sure that you comment on your results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0o6QA5C6JqQt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e13'></a>\n",
        "### Exercise 13 (points 5)\n",
        "\n",
        "Extract TF-IDF (or other, your choice) features for the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**comments**\n",
        "\n",
        "VERY unclear what variables we have to use. Is it vocab, vocab_size? is it something else entirely? This I believe should be specified, as it would save us a lot of time.\n",
        "\n",
        "vocab somehow has a size of ~4000, but the train set is twice as large, do i need to make a new variable for this? the current code is great but runs into errors because some words simply don't exist in this 'vocab' variable? how can that be the case, why is the variable called vocab then? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "VyVwDKbHJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE\n",
        "\n",
        "docs1 = [i for i in tweet_ds['train']['clean']] # ?\n",
        "docs2 = [i for i in tweet_ds['test']['clean']]\n",
        "docs3 = [i for i in tweet_ds['validation']['clean']]\n",
        "docs = docs1 + docs2 + docs3\n",
        "num_docs = len(docs)\n",
        "sentence = ' '.join(docs)\n",
        "vocab = get_bag_of_words(sentence)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# here is the skeleton of a function you can fill in\n",
        "def calculate_tf_idf(example):\n",
        "    \"\"\"\n",
        "    Calculates the TF-IDF and encodes it as a numpy array of the size equal to the vocab\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: updated example with 'tf_idf' column\n",
        "\n",
        "    \"\"\"\n",
        "    text = example['clean']\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # create numpy array of the CORRECT size (you will have to change the following line)\n",
        "    tf_idf_numpy = np.zeros(vocab_size, dtype=float)\n",
        "\n",
        "    bow_text = get_bag_of_words(text)\n",
        "\n",
        "    tf = {word: (bow_text[word] / num_docs) for word in bow_text.keys()} # get dictionary using comprehension\n",
        "\n",
        "    num_documents = len(text) # assuming this will work with text\n",
        "    contains_word = Counter() # count of documents containing word\n",
        "\n",
        "    for word in bow_text.keys():\n",
        "        for doc in text: # use text or tweets_ds -> unclear\n",
        "            if word in doc.split(): # check if word appears in the doc\n",
        "                contains_word[word] += 1\n",
        "\n",
        "    idf = {word: np.log(num_documents / (contains_word[word] + 1)) for word in bow_text.keys()}\n",
        "\n",
        "    # multiply the things together\n",
        "    tf_idf = {word: tf[word] * idf[word] for word in bow_text.keys()}\n",
        "\n",
        "    # get array populated\n",
        "    # it is VERY time consuming trying to understand what variables to use here, please next lab more clear\n",
        "    for word, val in tf_idf.items():\n",
        "        idx = list(vocab.keys()).index(word)\n",
        "        tf_idf_numpy[idx] = val\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n",
        "\n",
        "    example['tf_idf'] = tf_idf_numpy\n",
        "    return example\n",
        "\n",
        "### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsaZw6xws6v-"
      },
      "source": [
        "A result of the function being applied to a single example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "eSGFYtnTJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.         0.00041434 0.00065916 0.00082867 0.00124301]\n"
          ]
        }
      ],
      "source": [
        "print(np.unique(calculate_tf_idf(tweet_ds['train'][9])['tf_idf'])) # this will show that the thing works just fine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfBWo0IctAJU"
      },
      "source": [
        "Apply the function to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "x-12IfEsJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 8993/8993 [00:22<00:00, 402.97 examples/s]\n",
            "Map: 100%|██████████| 2970/2970 [00:07<00:00, 402.61 examples/s]\n",
            "Map: 100%|██████████| 999/999 [00:02<00:00, 337.71 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.map(calculate_tf_idf)\n",
        "\n",
        "print(tweet_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCMZfS9Htdhl"
      },
      "source": [
        "Extract the features for each subset. Labels (```y_train```, etc.) are the same as for the previous exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "yUsD8bBgtoL9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8993, 23005)\n",
            "(8993,)\n",
            "(999, 23005)\n",
            "(999,)\n",
            "(2970, 23005)\n",
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "X_train_tf_idf = tweet_ds['train']['tf_idf']\n",
        "\n",
        "print(X_train_tf_idf.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "X_valid_tf_idf = tweet_ds['validation']['tf_idf']\n",
        "\n",
        "print(X_valid_tf_idf.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "X_test_tf_idf = tweet_ds['test']['tf_idf']\n",
        "\n",
        "print(X_test_tf_idf.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3W2Gd_SNJqQt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e14'></a>\n",
        "### Exercise 14 (points 10)\n",
        "\n",
        "Train a classifier of your choice on TF-IDF features. Evaluate it on the validation and test dataset and compare with the previous experiments. As usual, provide explanations and insights to your analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "# from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = xgb.XGBClassifier(\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    min_child_weight=1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    n_estimators=500,  # Set a large value to ensure early stopping can occur\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    gamma=0.8\n",
        ")\n",
        "\n",
        "tf_idf_model = clf.fit(X_train_tf_idf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.838688117205769\n",
            "validation score:\n",
            "f1: 0.7044551113777845\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.80      0.76       572\n",
            "           1       0.69      0.61      0.65       427\n",
            "\n",
            "    accuracy                           0.72       999\n",
            "   macro avg       0.71      0.70      0.70       999\n",
            "weighted avg       0.71      0.72      0.71       999\n",
            "\n",
            "test score:\n",
            "f1: 0.49068298635356167\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.25      0.38      1718\n",
            "           1       0.46      0.88      0.61      1252\n",
            "\n",
            "    accuracy                           0.52      2970\n",
            "   macro avg       0.60      0.57      0.49      2970\n",
            "weighted avg       0.63      0.52      0.47      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# use the classifier found using the grid search above\n",
        "\n",
        "tf_idf_train_prediction = tf_idf_model.predict(X_train_tf_idf)\n",
        "tf_idf_valid_prediction = tf_idf_model.predict(X_valid_tf_idf)\n",
        "tf_idf_test_prediction = tf_idf_model.predict(X_test_tf_idf)\n",
        "\n",
        "print('training score:')\n",
        "print_scores(y_train, tf_idf_train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, tf_idf_valid_prediction, report=True)\n",
        "print('test score:')\n",
        "print_scores(y_test, tf_idf_test_prediction, report = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "dEkcLpotJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.9992013668525568\n",
            "validation score:\n",
            "f1: 0.709708642485975\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.78      0.76       572\n",
            "           1       0.68      0.63      0.66       427\n",
            "\n",
            "    accuracy                           0.72       999\n",
            "   macro avg       0.71      0.71      0.71       999\n",
            "weighted avg       0.72      0.72      0.72       999\n",
            "\n",
            "test score:\n",
            "f1: 0.48745281008523383\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.25      0.37      1718\n",
            "           1       0.46      0.87      0.60      1252\n",
            "\n",
            "    accuracy                           0.51      2970\n",
            "   macro avg       0.59      0.56      0.49      2970\n",
            "weighted avg       0.61      0.51      0.47      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "tf_idf_model = clf.fit(X_train_tf_idf, y_train)\n",
        "\n",
        "tf_idf_train_prediction = tf_idf_model.predict(X_train_tf_idf)\n",
        "tf_idf_valid_prediction = tf_idf_model.predict(X_valid_tf_idf)\n",
        "tf_idf_test_prediction = tf_idf_model.predict(X_test_tf_idf)\n",
        "\n",
        "print('training score:')\n",
        "print_scores(y_train, tf_idf_train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, tf_idf_valid_prediction, report=True)\n",
        "print('test score:')\n",
        "print_scores(y_test, tf_idf_test_prediction, report = True)\n",
        "\n",
        "### MAKE SURE TO CHECK TRAIN/VALID/TEST SET PERFORMANCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP05s4Ye3-La"
      },
      "source": [
        "**ANSWERS**\n",
        "\n",
        "I chose to use 2 models for this task: XGBoost and Random Forest.\n",
        "\n",
        "Using XGBoost for this task seemed to be a good idea, as I thought it would give me a very good performance for this task, given XGBoost is usually good at classification tasks. However, as you can see from my results, the f1 score on the test set is only about 0.486, making my model even worse than a random guessing model. An unexpected result, given the otherwise acceptable f1 scores for both the training and validation sets. XGBoost as it turns out, did not do well at all with the TF-IDF features.\n",
        "\n",
        "Random Forest, as expected, shows an almost perfect accuracy on the training set, as these models tend to overfit the data quite strongly. The f1 score on the validation set is decent, at about 0.7, therefore, like before, I expected a f1 score of about 0.6 for the test set, but got slightly below 0.5 once again.\n",
        "\n",
        "It seems the TF-IDF features and the way I used my regex to clean the dataset do not produce any good classifiers. Maybe adding more features will help the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEgIPt-WrAR6"
      },
      "source": [
        "<a name='9'></a>\n",
        "## 9. Adding Handcrafted Features\n",
        "\n",
        "In this section you will use either Bag-of-Words or TF-IDF (whichever performed better) but with a twist. You will add handcrafted features designed by you: Recall that in lectures we discussed how we can expand Bag-of-Words for this purpose (e.g. to include features like # of positive words or # of negative words). They will be concatenated with BOW or TF-IDF numpy array and used to train the classifier of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIjHBU8lr0Ks"
      },
      "source": [
        "<a name='e15'></a>\n",
        "### Exercise 15 (points 10)\n",
        "\n",
        "Fill in the following function and write your own features (e.g. number of hashtags in the example)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# i will use a special list of bad words i found online.\n",
        "\n",
        "https://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List of words: ['abbo', 'abo', 'abortion', 'abuse', 'addict', 'addicts', 'adult', 'africa', 'african', 'alla', 'allah', 'alligatorbait', 'amateur', 'american', 'anal', 'analannie', 'analsex', 'angie', 'angry', 'anus', 'arab', 'arabs', 'areola', 'argie', 'aroused', 'arse', 'arsehole', 'asian', 'ass', 'assassin', 'assassinate', 'assassination', 'assault', 'assbagger', 'assblaster', 'assclown', 'asscowboy', 'asses', 'assfuck', 'assfucker', 'asshat', 'asshole', 'assholes', 'asshore', 'assjockey', 'asskiss', 'asskisser', 'assklown', 'asslick', 'asslicker', 'asslover', 'assman', 'assmonkey', 'assmunch', 'assmuncher', 'asspacker', 'asspirate', 'asspuppies', 'assranger', 'asswhore', 'asswipe', 'athletesfoot', 'attack', 'australian', 'babe', 'babies', 'backdoor', 'backdoorman', 'backseat', 'badfuck', 'balllicker', 'balls', 'ballsack', 'banging', 'baptist', 'barelylegal', 'barf', 'barface', 'barfface', 'bast', 'bastard', 'bazongas', 'bazooms', 'beaner', 'beast', 'beastality', 'beastial', 'beastiality', 'beatoff', 'beat-off', 'beatyourmeat', 'beaver', 'bestial', 'bestiality', 'bi', 'biatch', 'bible', 'bicurious', 'bigass', 'bigbastard', 'bigbutt', 'bigger', 'bisexual', 'bi-sexual', 'bitch', 'bitcher', 'bitches', 'bitchez', 'bitchin', 'bitching', 'bitchslap', 'bitchy', 'biteme', 'black', 'blackman', 'blackout', 'blacks', 'blind', 'blow', 'blowjob', 'boang', 'bogan', 'bohunk', 'bollick', 'bollock', 'bomb', 'bombers', 'bombing', 'bombs', 'bomd', 'bondage', 'boner', 'bong', 'boob', 'boobies', 'boobs', 'booby', 'boody', 'boom', 'boong', 'boonga', 'boonie', 'booty', 'bootycall', 'bountybar', 'bra', 'brea5t', 'breast', 'breastjob', 'breastlover', 'breastman', 'brothel', 'bugger', 'buggered', 'buggery', 'bullcrap', 'bulldike', 'bulldyke', 'bullshit', 'bumblefuck', 'bumfuck', 'bunga', 'bunghole', 'buried', 'burn', 'butchbabes', 'butchdike', 'butchdyke', 'butt', 'buttbang', 'butt-bang', 'buttface', 'buttfuck', 'butt-fuck', 'buttfucker', 'butt-fucker', 'buttfuckers', 'butt-fuckers', 'butthead', 'buttman', 'buttmunch', 'buttmuncher', 'buttpirate', 'buttplug', 'buttstain', 'byatch', 'cacker', 'cameljockey', 'cameltoe', 'canadian', 'cancer', 'carpetmuncher', 'carruth', 'catholic', 'catholics', 'cemetery', 'chav', 'cherrypopper', 'chickslick', \"children's\", 'chin', 'chinaman', 'chinamen', 'chinese', 'chink', 'chinky', 'choad', 'chode', 'christ', 'christian', 'church', 'cigarette', 'cigs', 'clamdigger', 'clamdiver', 'clit', 'clitoris', 'clogwog', 'cocaine', 'cock', 'cockblock', 'cockblocker', 'cockcowboy', 'cockfight', 'cockhead', 'cockknob', 'cocklicker', 'cocklover', 'cocknob', 'cockqueen', 'cockrider', 'cocksman', 'cocksmith', 'cocksmoker', 'cocksucer', 'cocksuck', 'cocksucked', 'cocksucker', 'cocksucking', 'cocktail', 'cocktease', 'cocky', 'cohee', 'coitus', 'color', 'colored', 'coloured', 'commie', 'communist', 'condom', 'conservative', 'conspiracy', 'coolie', 'cooly', 'coon', 'coondog', 'copulate', 'cornhole', 'corruption', 'cra5h', 'crabs', 'crack', 'crackpipe', 'crackwhore', 'crack-whore', 'crap', 'crapola', 'crapper', 'crappy', 'crash', 'creamy', 'crime', 'crimes', 'criminal', 'criminals', 'crotch', 'crotchjockey', 'crotchmonkey', 'crotchrot', 'cum', 'cumbubble', 'cumfest', 'cumjockey', 'cumm', 'cummer', 'cumming', 'cumquat', 'cumqueen', 'cumshot', 'cunilingus', 'cunillingus', 'cunn', 'cunnilingus', 'cunntt', 'cunt', 'cunteyed', 'cuntfuck', 'cuntfucker', 'cuntlick', 'cuntlicker', 'cuntlicking', 'cuntsucker', 'cybersex', 'cyberslimer', 'dago', 'dahmer', 'dammit', 'damn', 'damnation', 'damnit', 'darkie', 'darky', 'datnigga', 'dead', 'deapthroat', 'death', 'deepthroat', 'defecate', 'dego', 'demon', 'deposit', 'desire', 'destroy', 'deth', 'devil', 'devilworshipper', 'dick', 'dickbrain', 'dickforbrains', 'dickhead', 'dickless', 'dicklick', 'dicklicker', 'dickman', 'dickwad', 'dickweed', 'diddle', 'die', 'died', 'dies', 'dike', 'dildo', 'dingleberry', 'dink', 'dipshit', 'dipstick', 'dirty', 'disease', 'diseases', 'disturbed', 'dive', 'dix', 'dixiedike', 'dixiedyke', 'doggiestyle', 'doggystyle', 'dong', 'doodoo', 'doo-doo', 'doom', 'dope', 'dragqueen', 'dragqween', 'dripdick', 'drug', 'drunk', 'drunken', 'dumb', 'dumbass', 'dumbbitch', 'dumbfuck', 'dyefly', 'dyke', 'easyslut', 'eatballs', 'eatme', 'eatpussy', 'ecstacy', 'ejaculate', 'ejaculated', 'ejaculating', 'ejaculation', 'enema', 'enemy', 'erect', 'erection', 'ero', 'escort', 'ethiopian', 'ethnic', 'european', 'evl', 'excrement', 'execute', 'executed', 'execution', 'executioner', 'explosion', 'facefucker', 'faeces', 'fag', 'fagging', 'faggot', 'fagot', 'failed', 'failure', 'fairies', 'fairy', 'faith', 'fannyfucker', 'fart', 'farted', 'farting', 'farty', 'fastfuck', 'fat', 'fatah', 'fatass', 'fatfuck', 'fatfucker', 'fatso', 'fckcum', 'fear', 'feces', 'felatio', 'felch', 'felcher', 'felching', 'fellatio', 'feltch', 'feltcher', 'feltching', 'fetish', 'fight', 'filipina', 'filipino', 'fingerfood', 'fingerfuck', 'fingerfucked', 'fingerfucker', 'fingerfuckers', 'fingerfucking', 'fire', 'firing', 'fister', 'fistfuck', 'fistfucked', 'fistfucker', 'fistfucking', 'fisting', 'flange', 'flasher', 'flatulence', 'floo', 'flydie', 'flydye', 'fok', 'fondle', 'footaction', 'footfuck', 'footfucker', 'footlicker', 'footstar', 'fore', 'foreskin', 'forni', 'fornicate', 'foursome', 'fourtwenty', 'fraud', 'freakfuck', 'freakyfucker', 'freefuck', 'fu', 'fubar', 'fuc', 'fucck', 'fuck', 'fucka', 'fuckable', 'fuckbag', 'fuckbuddy', 'fucked', 'fuckedup', 'fucker', 'fuckers', 'fuckface', 'fuckfest', 'fuckfreak', 'fuckfriend', 'fuckhead', 'fuckher', 'fuckin', 'fuckina', 'fucking', 'fuckingbitch', 'fuckinnuts', 'fuckinright', 'fuckit', 'fuckknob', 'fuckme', 'fuckmehard', 'fuckmonkey', 'fuckoff', 'fuckpig', 'fucks', 'fucktard', 'fuckwhore', 'fuckyou', 'fudgepacker', 'fugly', 'fuk', 'fuks', 'funeral', 'funfuck', 'fungus', 'fuuck', 'gangbang', 'gangbanged', 'gangbanger', 'gangsta', 'gatorbait', 'gay', 'gaymuthafuckinwhore', 'gaysex', 'geez', 'geezer', 'geni', 'genital', 'german', 'getiton', 'gin', 'ginzo', 'gipp', 'girls', 'givehead', 'glazeddonut', 'gob', 'god', 'godammit', 'goddamit', 'goddammit', 'goddamn', 'goddamned', 'goddamnes', 'goddamnit', 'goddamnmuthafucker', 'goldenshower', 'gonorrehea', 'gonzagas', 'gook', 'gotohell', 'goy', 'goyim', 'greaseball', 'gringo', 'groe', 'gross', 'grostulation', 'gubba', 'gummer', 'gun', 'gyp', 'gypo', 'gypp', 'gyppie', 'gyppo', 'gyppy', 'hamas', 'handjob', 'hapa', 'harder', 'hardon', 'harem', 'headfuck', 'headlights', 'hebe', 'heeb', 'hell', 'henhouse', 'heroin', 'herpes', 'heterosexual', 'hijack', 'hijacker', 'hijacking', 'hillbillies', 'hindoo', 'hiscock', 'hitler', 'hitlerism', 'hitlerist', 'hiv', 'ho', 'hobo', 'hodgie', 'hoes', 'hole', 'holestuffer', 'homicide', 'homo', 'homobangers', 'homosexual', 'honger', 'honk', 'honkers', 'honkey', 'honky', 'hook', 'hooker', 'hookers', 'hooters', 'hore', 'hork', 'horn', 'horney', 'horniest', 'horny', 'horseshit', 'hosejob', 'hoser', 'hostage', 'hotdamn', 'hotpussy', 'hottotrot', 'hummer', 'husky', 'hussy', 'hustler', 'hymen', 'hymie', 'iblowu', 'idiot', 'ikey', 'illegal', 'incest', 'insest', 'intercourse', 'interracial', 'intheass', 'inthebuff', 'israel', 'israeli', \"israel's\", 'italiano', 'itch', 'jackass', 'jackoff', 'jackshit', 'jacktheripper', 'jade', 'jap', 'japanese', 'japcrap', 'jebus', 'jeez', 'jerkoff', 'jesus', 'jesuschrist', 'jew', 'jewish', 'jiga', 'jigaboo', 'jigg', 'jigga', 'jiggabo', 'jigger', 'jiggy', 'jihad', 'jijjiboo', 'jimfish', 'jism', 'jiz', 'jizim', 'jizjuice', 'jizm', 'jizz', 'jizzim', 'jizzum', 'joint', 'juggalo', 'jugs', 'junglebunny', 'kaffer', 'kaffir', 'kaffre', 'kafir', 'kanake', 'kid', 'kigger', 'kike', 'kill', 'killed', 'killer', 'killing', 'kills', 'kink', 'kinky', 'kissass', 'kkk', 'knife', 'knockers', 'kock', 'kondum', 'koon', 'kotex', 'krap', 'krappy', 'kraut', 'kum', 'kumbubble', 'kumbullbe', 'kummer', 'kumming', 'kumquat', 'kums', 'kunilingus', 'kunnilingus', 'kunt', 'ky', 'kyke', 'lactate', 'laid', 'lapdance', 'latin', 'lesbain', 'lesbayn', 'lesbian', 'lesbin', 'lesbo', 'lez', 'lezbe', 'lezbefriends', 'lezbo', 'lezz', 'lezzo', 'liberal', 'libido', 'licker', 'lickme', 'lies', 'limey', 'limpdick', 'limy', 'lingerie', 'liquor', 'livesex', 'loadedgun', 'lolita', 'looser', 'loser', 'lotion', 'lovebone', 'lovegoo', 'lovegun', 'lovejuice', 'lovemuscle', 'lovepistol', 'loverocket', 'lowlife', 'lsd', 'lubejob', 'lucifer', 'luckycammeltoe', 'lugan', 'lynch', 'macaca', 'mad', 'mafia', 'magicwand', 'mams', 'manhater', 'manpaste', 'marijuana', 'mastabate', 'mastabater', 'masterbate', 'masterblaster', 'mastrabator', 'masturbate', 'masturbating', 'mattressprincess', 'meatbeatter', 'meatrack', 'meth', 'mexican', 'mgger', 'mggor', 'mickeyfinn', 'mideast', 'milf', 'minority', 'mockey', 'mockie', 'mocky', 'mofo', 'moky', 'moles', 'molest', 'molestation', 'molester', 'molestor', 'moneyshot', 'mooncricket', 'mormon', 'moron', 'moslem', 'mosshead', 'mothafuck', 'mothafucka', 'mothafuckaz', 'mothafucked', 'mothafucker', 'mothafuckin', 'mothafucking', 'mothafuckings', 'motherfuck', 'motherfucked', 'motherfucker', 'motherfuckin', 'motherfucking', 'motherfuckings', 'motherlovebone', 'muff', 'muffdive', 'muffdiver', 'muffindiver', 'mufflikcer', 'mulatto', 'muncher', 'munt', 'murder', 'murderer', 'muslim', 'naked', 'narcotic', 'nasty', 'nastybitch', 'nastyho', 'nastyslut', 'nastywhore', 'nazi', 'necro', 'negro', 'negroes', 'negroid', \"negro's\", 'nig', 'niger', 'nigerian', 'nigerians', 'nigg', 'nigga', 'niggah', 'niggaracci', 'niggard', 'niggarded', 'niggarding', 'niggardliness', \"niggardliness's\", 'niggardly', 'niggards', \"niggard's\", 'niggaz', 'nigger', 'niggerhead', 'niggerhole', 'niggers', \"nigger's\", 'niggle', 'niggled', 'niggles', 'niggling', 'nigglings', 'niggor', 'niggur', 'niglet', 'nignog', 'nigr', 'nigra', 'nigre', 'nip', 'nipple', 'nipplering', 'nittit', 'nlgger', 'nlggor', 'nofuckingway', 'nook', 'nookey', 'nookie', 'noonan', 'nooner', 'nude', 'nudger', 'nuke', 'nutfucker', 'nymph', 'ontherag', 'oral', 'orga', 'orgasim', 'orgasm', 'orgies', 'orgy', 'osama', 'paki', 'palesimian', 'palestinian', 'pansies', 'pansy', 'panti', 'panties', 'payo', 'pearlnecklace', 'peck', 'pecker', 'peckerwood', 'pee', 'peehole', 'pee-pee', 'peepshow', 'peepshpw', 'pendy', 'penetration', 'peni5', 'penile', 'penis', 'penises', 'penthouse', 'period', 'perv', 'phonesex', 'phuk', 'phuked', 'phuking', 'phukked', 'phukking', 'phungky', 'phuq', 'pi55', 'picaninny', 'piccaninny', 'pickaninny', 'piker', 'pikey', 'piky', 'pimp', 'pimped', 'pimper', 'pimpjuic', 'pimpjuice', 'pimpsimp', 'pindick', 'piss', 'pissed', 'pisser', 'pisses', 'pisshead', 'pissin', 'pissing', 'pissoff', 'pistol', 'pixie', 'pixy', 'playboy', 'playgirl', 'pocha', 'pocho', 'pocketpool', 'pohm', 'polack', 'pom', 'pommie', 'pommy', 'poo', 'poon', 'poontang', 'poop', 'pooper', 'pooperscooper', 'pooping', 'poorwhitetrash', 'popimp', 'porchmonkey', 'porn', 'pornflick', 'pornking', 'porno', 'pornography', 'pornprincess', 'pot', 'poverty', 'premature', 'pric', 'prick', 'prickhead', 'primetime', 'propaganda', 'pros', 'prostitute', 'protestant', 'pu55i', 'pu55y', 'pube', 'pubic', 'pubiclice', 'pud', 'pudboy', 'pudd', 'puddboy', 'puke', 'puntang', 'purinapricness', 'puss', 'pussie', 'pussies', 'pussy', 'pussycat', 'pussyeater', 'pussyfucker', 'pussylicker', 'pussylips', 'pussylover', 'pussypounder', 'pusy', 'quashie', 'queef', 'queer', 'quickie', 'quim', 'ra8s', 'rabbi', 'racial', 'racist', 'radical', 'radicals', 'raghead', 'randy', 'rape', 'raped', 'raper', 'rapist', 'rearend', 'rearentry', 'rectum', 'redlight', 'redneck', 'reefer', 'reestie', 'refugee', 'reject', 'remains', 'rentafuck', 'republican', 'rere', 'retard', 'retarded', 'ribbed', 'rigger', 'rimjob', 'rimming', 'roach', 'robber', 'roundeye', 'rump', 'russki', 'russkie', 'sadis', 'sadom', 'samckdaddy', 'sandm', 'sandnigger', 'satan', 'scag', 'scallywag', 'scat', 'schlong', 'screw', 'screwyou', 'scrotum', 'scum', 'semen', 'seppo', 'servant', 'sex', 'sexed', 'sexfarm', 'sexhound', 'sexhouse', 'sexing', 'sexkitten', 'sexpot', 'sexslave', 'sextogo', 'sextoy', 'sextoys', 'sexual', 'sexually', 'sexwhore', 'sexy', 'sexymoma', 'sexy-slim', 'shag', 'shaggin', 'shagging', 'shat', 'shav', 'shawtypimp', 'sheeney', 'shhit', 'shinola', 'shit', 'shitcan', 'shitdick', 'shite', 'shiteater', 'shited', 'shitface', 'shitfaced', 'shitfit', 'shitforbrains', 'shitfuck', 'shitfucker', 'shitfull', 'shithapens', 'shithappens', 'shithead', 'shithouse', 'shiting', 'shitlist', 'shitola', 'shitoutofluck', 'shits', 'shitstain', 'shitted', 'shitter', 'shitting', 'shitty', 'shoot', 'shooting', 'shortfuck', 'showtime', 'sick', 'sissy', 'sixsixsix', 'sixtynine', 'sixtyniner', 'skank', 'skankbitch', 'skankfuck', 'skankwhore', 'skanky', 'skankybitch', 'skankywhore', 'skinflute', 'skum', 'skumbag', 'slant', 'slanteye', 'slapper', 'slaughter', 'slav', 'slave', 'slavedriver', 'sleezebag', 'sleezeball', 'slideitin', 'slime', 'slimeball', 'slimebucket', 'slopehead', 'slopey', 'slopy', 'slut', 'sluts', 'slutt', 'slutting', 'slutty', 'slutwear', 'slutwhore', 'smack', 'smackthemonkey', 'smut', 'snatch', 'snatchpatch', 'snigger', 'sniggered', 'sniggering', 'sniggers', \"snigger's\", 'sniper', 'snot', 'snowback', 'snownigger', 'sob', 'sodom', 'sodomise', 'sodomite', 'sodomize', 'sodomy', 'sonofabitch', 'sonofbitch', 'sooty', 'sos', 'soviet', 'spaghettibender', 'spaghettinigger', 'spank', 'spankthemonkey', 'sperm', 'spermacide', 'spermbag', 'spermhearder', 'spermherder', 'spic', 'spick', 'spig', 'spigotty', 'spik', 'spit', 'spitter', 'splittail', 'spooge', 'spreadeagle', 'spunk', 'spunky', 'squaw', 'stagg', 'stiffy', 'strapon', 'stringer', 'stripclub', 'stroke', 'stroking', 'stupid', 'stupidfuck', 'stupidfucker', 'suck', 'suckdick', 'sucker', 'suckme', 'suckmyass', 'suckmydick', 'suckmytit', 'suckoff', 'suicide', 'swallow', 'swallower', 'swalow', 'swastika', 'sweetness', 'syphilis', 'taboo', 'taff', 'tampon', 'tang', 'tantra', 'tarbaby', 'tard', 'teat', 'terror', 'terrorist', 'teste', 'testicle', 'testicles', 'thicklips', 'thirdeye', 'thirdleg', 'threesome', 'threeway', 'timbernigger', 'tinkle', 'tit', 'titbitnipply', 'titfuck', 'titfucker', 'titfuckin', 'titjob', 'titlicker', 'titlover', 'tits', 'tittie', 'titties', 'titty', 'tnt', 'toilet', 'tongethruster', 'tongue', 'tonguethrust', 'tonguetramp', 'tortur', 'torture', 'tosser', 'towelhead', 'trailertrash', 'tramp', 'trannie', 'tranny', 'transexual', 'transsexual', 'transvestite', 'triplex', 'trisexual', 'trojan', 'trots', 'tuckahoe', 'tunneloflove', 'turd', 'turnon', 'twat', 'twink', 'twinkie', 'twobitwhore', 'uck', 'uk', 'unfuckable', 'upskirt', 'uptheass', 'upthebutt', 'urinary', 'urinate', 'urine', 'usama', 'uterus', 'vagina', 'vaginal', 'vatican', 'vibr', 'vibrater', 'vibrator', 'vietcong', 'violence', 'virgin', 'virginbreaker', 'vomit', 'vulva', 'wab', 'wank', 'wanker', 'wanking', 'waysted', 'weapon', 'weenie', 'weewee', 'welcher', 'welfare', 'wetb', 'wetback', 'wetspot', 'whacker', 'whash', 'whigger', 'whiskey', 'whiskeydick', 'whiskydick', 'whit', 'whitenigger', 'whites', 'whitetrash', 'whitey', 'whiz', 'whop', 'whore', 'whorefucker', 'whorehouse', 'wigger', 'willie', 'williewanker', 'willy', 'wn', 'wog', \"women's\", 'wop', 'wtf', 'wuss', 'wuzzie', 'xtc', 'xxx', 'yankee', 'yellowman', 'zigabo', 'zipperhead']\n"
          ]
        }
      ],
      "source": [
        "# URL of the text file\n",
        "url = \"https://www.cs.cmu.edu/~biglou/resources/bad-words.txt\"\n",
        "\n",
        "# Fetch the content of the text file\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Extract words from the text content\n",
        "    hate_words = response.text.split()  # Assuming words are separated by whitespace\n",
        "    print(\"List of words:\", hate_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "IjHMNWmBrzcm"
      },
      "outputs": [],
      "source": [
        "hate_words_len = len(hate_words) # i use for some kind of ratio\n",
        "\n",
        "def calculate_handcrafted_features(example):\n",
        "    \"\"\"\n",
        "    Calculates the handcrafted features for a given example\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: updated example with 'handcrafted_features' column\n",
        "\n",
        "    \"\"\"\n",
        "    text = example['clean']\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    bow = get_bag_of_words(text) # dictionary (counter)\n",
        "    \n",
        "    hate_count = 0 # +1 whenever hate is present\n",
        "    hash_count = 0 # hashtag counter\n",
        "    person_count = 0 # person counter\n",
        "    double_hate = 0 # sequence hate binary\n",
        "    exclamation_count = 0\n",
        "    bool_hate_context = False\n",
        "    \n",
        "    hate_sequence = [] # check if hate appears more than once in a row\n",
        "    \n",
        "    for word, count in bow.items(): # iterate over the sentences\n",
        "        if word in hate_words: # i check if word is in the hate words\n",
        "            hate_count += count # increase hate count by 1\n",
        "        if word == 'HASHTAG':\n",
        "            hash_count += count\n",
        "        if word == 'PERSON':\n",
        "            person_count += count\n",
        "        if len(hate_sequence) < 3: # i look at 2 words, not more\n",
        "            hate_sequence.append(word)\n",
        "        if len(hate_sequence) > 1: # 2 words present\n",
        "            double_hate = 1\n",
        "        if word == '!':\n",
        "            exclamation_count += 1\n",
        "        \n",
        "    # context for hate word, if positive, or not. There is a way to do this without loop (using loop above) but i can't see it now\n",
        "    for ix, word in enumerate(list(bow.keys())): # this just some lazy way of getting bow index\n",
        "        if ix > 1 and ix < (len(list(bow.keys())) - 2): # bounds for the context, -2 and 2\n",
        "            word_index = list(bow.keys()).index(word) # get index of word\n",
        "            context = [list(bow.keys())[word_index - 1], # this just keeps context, bounds described above\n",
        "                       list(bow.keys())[word_index - 2],\n",
        "                       list(bow.keys())[word_index + 1],\n",
        "                       list(bow.keys())[word_index + 2]]\n",
        "\n",
        "            np_context = np.array(context) # need in np array\n",
        "            np_hate_words = np.array(hate_words)\n",
        "            bool_hate_context = np.any(np.isin(np_context, np_hate_words)) # check if context in hate list\n",
        "\n",
        "    # UPDATE this number to the number of features you intend to include\n",
        "    num_of_features = 6\n",
        "\n",
        "    # create numpy array of the size equal to the number of features\n",
        "    handcrafted_features = np.zeros(num_of_features, dtype=float)\n",
        "\n",
        "    handcrafted_features[0] = hate_count # first feature is the hate ratio\n",
        "    handcrafted_features[1] = hash_count # second feature is the number of hashtags\n",
        "    handcrafted_features[2] = person_count # third feature is the number of user tags\n",
        "    handcrafted_features[3] = double_hate\n",
        "    handcrafted_features[4] = exclamation_count\n",
        "    handcrafted_features[5] = bool_hate_context\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n",
        "\n",
        "    example['handcrafted_features'] = handcrafted_features\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPYXFT-rtEex"
      },
      "source": [
        "Let's see the result of the function applied to a single example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "nO8FYdhdtENt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on you…', 'label': 0, 'clean': 'PERSON nice new signage . are you not concerned by beatlemania style hysterical crowds crongregating on you .', 'bow': array([1, 0, 0, ..., 0, 0, 0]), 'tf_idf': array([0.00036193, 0.00036193, 0.00036193, ..., 0.        , 0.        ,\n",
            "       0.        ], dtype=float32), 'handcrafted_features': array([0., 0., 1., 1., 0., 0.])}\n"
          ]
        }
      ],
      "source": [
        "print(calculate_handcrafted_features(tweet_ds['train'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7svF_Q1tMv4"
      },
      "source": [
        "Now, let's apply it to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "qTC1G_24tQ9X"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 8993/8993 [00:43<00:00, 204.99 examples/s]\n",
            "Map: 100%|██████████| 2970/2970 [00:14<00:00, 209.98 examples/s]\n",
            "Map: 100%|██████████| 999/999 [00:05<00:00, 178.64 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf', 'handcrafted_features'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf', 'handcrafted_features'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf', 'handcrafted_features'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.map(calculate_handcrafted_features)\n",
        "\n",
        "print(tweet_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8XT63Ant7u4"
      },
      "source": [
        "We can now extract the handcrafted features and combine with TF-IDF featuers (or any of your choice from previously calculated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "-8-_kjBbuIAc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8993, 23011)\n",
            "(8993,)\n",
            "(999, 23011)\n",
            "(999,)\n",
            "(2970, 23011)\n",
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "X_train_handcrafted = tweet_ds['train']['handcrafted_features']\n",
        "X_train_combined = np.concatenate((X_train_tf_idf, X_train_handcrafted), axis=-1)\n",
        "print(X_train_combined.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "X_valid_handcrafted = tweet_ds['validation']['handcrafted_features']\n",
        "X_valid_combined = np.concatenate((X_valid_tf_idf, X_valid_handcrafted), axis=-1)\n",
        "print(X_valid_combined.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "X_test_handcrafted = tweet_ds['test']['handcrafted_features']\n",
        "X_test_combined = np.concatenate((X_test_tf_idf, X_test_handcrafted), axis=-1)\n",
        "print(X_test_combined.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdJIiZPQus9v"
      },
      "source": [
        "<a name='e16'></a>\n",
        "### Exercise 16 (points 10)\n",
        "\n",
        "Train the classifier of your choice (Naive Bayes, Logistic Regression, Neural Network, etc.) on the combined features. Evaluate it on the validation and test dataset and compare with the previous experiments. Did your handcrafted features improve the performance?\n",
        "\n",
        "This is an open ended question, meaning that you are free to implement different setups and judge the final performance. You do not need to include all your experiments but for every model you introduce make sure to check the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "reykd5k8u9OP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.6105510185555454\n",
            "validation score:\n",
            "f1: 0.6119441795960823\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.78      0.71       572\n",
            "           1       0.60      0.45      0.51       427\n",
            "\n",
            "    accuracy                           0.64       999\n",
            "   macro avg       0.63      0.61      0.61       999\n",
            "weighted avg       0.63      0.64      0.63       999\n",
            "\n",
            "test score:\n",
            "f1: 0.5640134685212801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.59      0.62      1718\n",
            "           1       0.49      0.54      0.51      1252\n",
            "\n",
            "    accuracy                           0.57      2970\n",
            "   macro avg       0.56      0.57      0.56      2970\n",
            "weighted avg       0.58      0.57      0.57      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE\n",
        "\n",
        "clf = LogisticRegression()\n",
        "\n",
        "rf = clf.fit(X_train_combined, y_train)\n",
        "\n",
        "train_prediction = rf.predict(X_train_combined)\n",
        "valid_prediction = rf.predict(X_valid_combined)\n",
        "test_prediction = rf.predict(X_test_combined)\n",
        "\n",
        "print('training score:')\n",
        "print_scores(y_train, train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, valid_prediction, report = True)\n",
        "print('test score:')\n",
        "print_scores(y_test, test_prediction, report = True)\n",
        "\n",
        "### MAKE SURE TO CHECK TRAIN/VALID/TEST SET PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1021,  697],\n",
              "       [ 580,  672]])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion_matrix(y_test, test_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ0dCxly5EuC"
      },
      "source": [
        "# ANSWER\n",
        "\n",
        "My new classifier uses a couple of new features I have defined myself:\n",
        "\n",
        "- count of hashtags in sentence\n",
        "- count of user tags in sentence\n",
        "- count of hate words in sentence\n",
        "- count of exclamation marks in sentence\n",
        "- context of word (up to 2 words before and 2 words after)\n",
        "- if hate words appear in pairs\n",
        "\n",
        "I have tried several models for this task: XGboost, RandomForest, GaussianNB, and Logistic Regression.\n",
        "\n",
        "Somehow, after adding my bespoke features, logistic regression both is the quickest to train, and performs the best, achieving 0.56 f1-score on the test set (note that all other methods give a f1-score of below 0.5 in all cases for the test set).\n",
        "\n",
        "- There are 1021 true negatives (TN).\n",
        "- There are 697 false positives (FP).\n",
        "- There are 580 false negatives (FN).\n",
        "- There are 672 true positives (TP).\n",
        "\n",
        "Overall, this model isn't bad at detecting non-hate speech, but seems to not do well in detecting actual hate speech. How much of this error can be attributed to wrong labeling is up for debate, but the model certainly can be improved in some ways. It is however the best model I have obtained on this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob6uxaVfjIP9"
      },
      "source": [
        "<a name='10'></a>\n",
        "## 10. Reflection, Bias, Fairness, Ethics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpu4SjMkwBxj"
      },
      "source": [
        "<a name='e17'></a>\n",
        "###  Exercise 17 (points 5)\n",
        "\n",
        "There are many different applications for such a hate-speech classification model. For example, social media platforms could use it in order to moderate relevant texts: this actually falls under the obligations of platforms to report on the moderating actions they have taken under the Data Services Act (DSA). You can check this database [here](https://transparency.dsa.ec.europa.eu/) and read more about the type of content being reported [here](https://arxiv.org/pdf/2404.02894.pdf).\n",
        "\n",
        "Systems used in the context of (lega)l decision-making or, more generally, systems that filter specific content (e.g. removing hate-speech comments) should be used with great care and in view of the potential interference\n",
        "with human rights, namely the right to free speech.\n",
        "\n",
        "Based on your experience from this lab (i.e. you saw how easy/difficult is to build such a task, you also saw how easy/difficult is to get good results and you also saw in which cases the model might not perform well enough) you are asked to reflect on the ethical use of such a classification model. More specifically, you are asked to reflect on the following questions (and feel free to expand your analysis).\n",
        "\n",
        "* Are there considerations for the (final) model you built? Did you make decisions (e.g. for the way data is processed) tht might affect the model?\n",
        "* What are the limitations of the model and your results?\n",
        "* Who can be potential relevant stakeholders of such a system? If you had to deploy such a model in practice, who would you need to consult before putting it into practice?\n",
        "* Who can benefit from such a model (and the analysis)? Who can be harmed? Who is excluded?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DQDz3Z15O6i"
      },
      "source": [
        "# ANSWER\n",
        "\n",
        "- considerations\n",
        "\n",
        "The model I have trained in the final exercise of this lab does not perform well (f1 0.56). The way I determine if a word is hate or not is also very simplistic, although I did use context to make that decision too, I am unable to get a good f1 score on the test set provided in this lab. I have noticed throughout the lab that many words which only show up in hate words are simply not hate words, so this is also very problematic. Additionally, I have only use normal substitution regex for this task, where I could have written some code for BPE, that might have helped me tokenize the dataset, because many words are misspelled to begin with, and therefore hate might just not be in most common hate words for such misspelled words.\n",
        "\n",
        "- limitations\n",
        "\n",
        "The model I trained cannot realistically be used for anything serious, it barely does better than random guessing. It does very decently on the validation set but simply cannot keep up on unseen data, which contains unseen words, unseen contexts..\n",
        "\n",
        "- stakeholders\n",
        "\n",
        "Potential stakeholders of such models are businesses involved in social media, for example: although platforms like X (twitter) claim to tolerate just about anything, should some laws banning hate speech on the platforms be passed, hate speech detection systems would save them a lot of money by preventing potential lawsuits. Deploying such models would require consulting lawyers, to see what exactly is being filtered, and to determine if the filters are overriding people's rights to free speech.\n",
        "\n",
        "- affected parties\n",
        "\n",
        "Minorities would maybe be positively affected by such systems, because good hate speech filters would mitigate some mass negative sentiment against them which can build up on social media platforms. Negatively affected parties may include people who make a living saying nonsense in their tweets, which is becoming a thing nowadays."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
